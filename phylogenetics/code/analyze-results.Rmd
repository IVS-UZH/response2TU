---
title: "Analysis of the phylogenetic aspects of Tarasov & Uyeda's 2019 Technical Comment on the Labiodentals paper"
author: "Dan Dediu"
date: "`r Sys.time()`"
output: 
  html_document: 
    df_print: kable
    highlight: textmate
    theme: readable
    toc: yes
    toc_depth: 6
    toc_float: yes
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, dev="jpeg", dpi=72);
```

```{r function definitions and libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(RevGadgets); # RevBayes library
library(coda);
library(ggplot2);
library(pander);
library(reshape2);
library(dplyr);
#library(bayesplot);

# Character names:
char_names <- c("bh"="b^h^", 
                "p"="p", 
                "w"="w", 
                "m"="m", 
                "gw"="g^w^", 
                "gwh"="g^wh^", 
                "kw"="k^w^", 
                "dh"="d^h^", 
                "ky"="k^j^", 
                "b"="b");


# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

# Parse a results (.log) file into its components and also return the important columns for this model:
parse.output.into.model.components <- function(model.file.name="LD_[br=RMC,rf=1,sr=MM]_B(1)_R1")
{
  # Parse the model file name into its components:
  model_prefix <- strsplit(model.file.name, "_[", fixed=TRUE)[[1]][1];
  
  tmp <- strsplit(strsplit(model.file.name, "_[", fixed=TRUE)[[1]][2], "]_", fixed=TRUE)[[1]][1];
  branch_rates_model <- strsplit(strsplit(tmp, "br=", fixed=TRUE)[[1]][2], ",", fixed=TRUE)[[1]][1];
  root_freq_model    <- strsplit(strsplit(tmp, "rf=", fixed=TRUE)[[1]][2], ",", fixed=TRUE)[[1]][1];
  site_rates_model   <- strsplit(strsplit(tmp, "sr=", fixed=TRUE)[[1]][2], ",", fixed=TRUE)[[1]][1];
  
  # Get the trees sample and the tree (if any):
  tmp <- strsplit(strsplit(model.file.name, "]_", fixed=TRUE)[[1]][2], "_", fixed=TRUE)[[1]][1];
  trees_sample_name <- strsplit(tmp, "(", fixed=TRUE)[[1]][1];
  which_tree        <- strsplit(strsplit(tmp, "(", fixed=TRUE)[[1]][2], ")", fixed=TRUE)[[1]][1];
  if( substring(which_tree, 1, 3) == "all" )
  {
    tree_sampling_method <- strsplit(which_tree, "+", fixed=TRUE)[[1]][2];
    which_tree <- "all";
  } else
  {
    tree_sampling_method <- NA;
  }
  
  
  # Depending on the precise model, different columns are of interest:
  columns.to.keep <- c("Posterior", "Likelihood", "Prior", "mean_rt");
  columns.to.keep <- c(columns.to.keep, 
                       if(which_tree == "all"){ "tr_id" });
  columns.to.keep <- c(columns.to.keep, 
                       if(root_freq_model == "DP"){ c("root_freq[1]", "root_freq[2]") });
  columns.to.keep <- c(columns.to.keep, 
                       if(site_rates_model == "1"){ "r12" }, 
                       if(site_rates_model == "MM"){ c("cats_r12[1]", "cats_r12[2]", "site_rates[1]", "site_rates[2]", "beta_cats_rates", "alpha_site_rates") });
  columns.to.keep <- c(columns.to.keep, 
                       if(branch_rates_model == "MC"){ NULL }, 
                       if(branch_rates_model == "ULR"){ c("n_cats", "ucln_mu", "ucln_sigma") },
                       if(branch_rates_model == "TU"){ c("n_cats", "concentration") },
                       if(branch_rates_model == "TUBR"){ c("n_cats", "concentration", "base_rate") },
                       if(branch_rates_model == "RMC"){ c("n_cats", "sigma2_root") });
  
  
  return (list("prefix"=model_prefix,
               "branch_rates_model"  =branch_rates_model,
               "root_freq_model"     =root_freq_model,
               "site_rates_model"    =site_rates_model,
               "trees_sample_name"   =trees_sample_name,
               "which_tree"          =which_tree,
               "tree_sampling_method"=tree_sampling_method,
               "columns.to.keep"     =columns.to.keep));
}


##########################################
#
# Convergence diagnostic (needs >1 runs)
#
##########################################

# Chains have converged for values close to 1.0 (see ?gelman.diag):
mcmc.diags.for.model <- function(model.file.name, thin=10, xz.compressed=TRUE)
{
  # Parse the model file name into its components:
  model.components <- parse.output.into.model.components(model.file.name);

  # Look for all the runs of this model:
  base_model_file_name <- paste0(model.components$prefix, "_[", 
                                 "br=", model.components$branch_rates_model, ",",
                                 "rf=", model.components$root_freq_model, ",",
                                 "sr=", model.components$site_rates_model, "]_", 
                                 model.components$trees_sample_name, "(", 
                                 ifelse(model.components$which_tree == "all", 
                                        paste0("all+", model.components$tree_sampling_method), 
                                        model.components$which_tree), ")", 
                                 "_R");
  all_files <- list.files(path="../output/", pattern=glob2rx(paste0("*.log",ifelse(xz.compressed,".xz",""))), full.names=FALSE, include.dirs=FALSE);
  all_files <- all_files[ grep(base_model_file_name, all_files, fixed=TRUE) ];
  
  if( length(all_files) > 0 )
  {
    # Load these runs:
    run_files <- lapply(all_files, function(s)
    {
      if( xz.compressed )
      {
        x <- read.table(xzfile(paste0("../output/",s)), header=TRUE, sep="\t", check.names=FALSE);
      } else
      {
        x <- read.table(paste0("../output/",s), header=TRUE, sep="\t", check.names=FALSE);
      }
      return (x[,model.components$columns.to.keep]);
    });
    
    # Check and remove constant columns (they mess up gelman.diag):
    const.cols <- colSums(do.call(rbind, lapply(run_files, function(x) vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1)))));
  
    # Cast it into the proper type:
    run_mcmc <- as.mcmc.list(lapply(run_files, function(x) as.mcmc(x, thin=thin)));
    
    # Transform them into MCMC objects (if the case):
    if( length(run_files) >= 2 &&  sum(const.cols == 0) > 0 )
    {
      # For glemann, keep only the non-constant variables:
      run_mcmc_g <- as.mcmc.list(lapply(run_files, function(x) as.mcmc(x[,which(const.cols == 0)], thin=thin)));
      
      # Compute and return the results:
      gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
    } else 
    {
      gdiag <- NULL;
    }
    
    # Return value:
    return (list("runs"=length(run_files), "gelman"=gdiag, "mcmc"=run_mcmc, "vars"=colnames(run_mcmc[[1]])));
  } else
  {
    # Return value:
    return (list("runs"=0, "gelman"=NULL, "mcmc"=NULL, "vars"=NULL));
  }
}



####################################
# Reconstruct the ancestral states
####################################

plot_anc_states_trees <- function(tree.files)
{
  chr.nums <- vapply(tree.files, function(s) as.numeric(strsplit(strsplit(s,".tree",fixed=TRUE)[[1]][1],"chr_",fixed=TRUE)[[1]][2]), numeric(1));
  tree.files <- tree.files[ order(chr.nums) ]; names(tree.files) <- chr.nums[ order(chr.nums) ];
  g <- list()
  for( i in seq_along(tree.files) )
  {
    s <- names(tree.files)[i];
    g[[length(g)+1]] <- plot_ancestral_states(tree.files[i], 
                                              #title=paste0("Character ",s[length(s)]), 
                                              title=names(char_names)[as.numeric(s[length(s)])],
                                              summary_statistic="MAP",
                                              tip_label_size=1,
                                              xlim_visible=NULL,
                                              node_label_size=0,
                                              show_posterior_legend=FALSE,
                                              node_size_range=c(2.5, 2.5),
                                              alpha=0.75);
  }
  multiplot(plotlist=g, cols=2);
}


# The ancestrat state of the root is easy to get as it is the *last* column in the files exported by the mnJointConditionalAncestralState() monitor:
root_anc_states <- function(model.name, model.short.name, section=3, xz.compressed=TRUE)
{
  # Read the whole ancestral states file:
  model.file.name <- paste0("../output/anc_states/", model.name, "_R1_anc_states.txt", ifelse(xz.compressed, ".xz", ""));
  if( file.exists(model.file.name) )
  {
    if( xz.compressed )
    {
      anc_file <- read.table(xzfile(model.file.name), header=TRUE, sep="\t", quote="");
    } else
    {
      anc_file <- read.table(model.file.name, header=TRUE, sep="\t", quote="");
    }
    
    # Keep only the first (Iteration) and last column (root states):
    anc_file <- anc_file[,c(1,ncol(anc_file))];
    
    # Split the last column into indepent characters:
    anc_root_vals <- as.data.frame(cbind(anc_file$Iteration, do.call(rbind, strsplit(as.character(anc_file[,2]), ",", fixed=TRUE))));
    #names(anc_root_vals) <- c("iteration", paste0("chr",1:(ncol(anc_root_vals)-1)));
    names(anc_root_vals) <- c("iteration", names(char_names));
    for( i in 1:ncol(anc_root_vals) ) anc_root_vals[,i] <- as.numeric(as.character(anc_root_vals[,i])); # convert them to numeric
    
    # Make it into and mcmc object:
    anc_mcmc <- mcmc(anc_root_vals[,-1], thin=10);
    
    # Print summaries and plot:
    cat(paste0(paste0(rep("#",section+1),collapse="")," Ancestral states at root\n\n"));
  
    plot(anc_mcmc);
  
    cat("The % of posterior reconstructions that estimate an ancestral '1' at the root for each character are:\n");
    tmp <- colSums(anc_root_vals[,-1]) / nrow(anc_root_vals) * 100;
    names(tmp) <- char_names;
    pander(tmp);
  }
}

# root_anc_states(model.name="LD_[br=MC,rf=1,sr=1]_B(all+TU)", model.short.name="M00")


####################################
# Branch rates for the relaxed molecular clock
####################################
#my_tree <- read.tree("../data/indoeuropean-B.tre")[1]
#my_output_file <- "../output/LD_tests_B(1)_R1.log"
#
#tree_plot <- plot_relaxed_branch_rates_tree(tree           = my_tree,
#                                            output_file    = my_output_file,
#                                            parameter_name = "branch_rates")
#
#ggsave("relaxed_BM.pdf", width=15, height=15, units="cm")



#######################################################
#
# Compute the Bayes Factors for a given set of models
#
#######################################################


# Load the marginal likelihood(s) for a given model:
load_marg_lkl <- function(model.name, xz.compressed=FALSE)
{
  model.file.name <- paste0("../output/pow_post/", model.name, "_marg_lkl.txt", ifelse(xz.compressed, ".xz", ""));
  if( file.exists(model.file.name) )
  {
    if( xz.compressed )
    {
      return (read.table(xzfile(model.file.name), header=TRUE, sep="\t", quote=""));
    } else
    {
      return (read.table(model.file.name, header=TRUE, sep="\t", quote=""));
    }
  } else
  {
    return (NULL);
  }
}

# For a set of marginal likelihoods, compute all pariwise (ln) Bayes Factors (with their interpretation):
compute_BF <- function(models.names)
{
  # Get the marginal likelihoods for each model:
  mlkls <- do.call(rbind, 
                   lapply(models.names, function(s) cbind("model"=s, load_marg_lkl(s))));
  mlkls <- mlkls[order(mlkls$Marginal_Likelihood, decreasing=TRUE),];
  
  # Prepare the pairwise BFs:
  BFs <- expand.grid("M1"=models.names, "M2"=models.names, "Method"=unique(as.character(mlkls$Method)), stringsAsFactors=FALSE);
  BFs <- BFs[ order(BFs$M1, BFs$M2, BFs$Method), ]; BFs <- BFs[ BFs$M1 < BFs$M2, ];

  # Compute and interpret the pairwise BFs:
  BFs <- cbind(BFs, do.call(rbind, lapply(1:nrow(BFs), function(i)
    {
      s1 <- (mlkls$model == BFs$M1[i] & mlkls$Method == BFs$Method[i]);
      s2 <- (mlkls$model == BFs$M2[i] & mlkls$Method == BFs$Method[i]);
      ml1 <- mlkls$Marginal_Likelihood[s1];
      ml2 <- mlkls$Marginal_Likelihood[s2];
      lnBF <- (ml1 - ml2);
      return (data.frame("ln.mar.lkl.M1"=ml1,
                         "ln.mar.lkl.M2"=ml2,
                         "lnBF.M1.vs.M2"=lnBF,
                         "supported"=ifelse(lnBF > 0, BFs$M1[i], BFs$M2[i]),
                         "support"=ifelse(abs(lnBF) > 4.6, "decisive",
                                          ifelse(abs(lnBF) > 2.3, "strong",
                                                 ifelse(abs(lnBF) > 1.16, "substantial", "barely worth mentioning")))));
  })));
  
  return (list("models"=mlkls, "BFs"=BFs));
}

## List all models with marginal likelihood estimates;
#models.names <- unlist(strsplit(list.files(path="../output/pow_post/", pattern=glob2rx("*_marg_lkl.txt"), full.names=FALSE, include.dirs=FALSE),
#                                "_marg_lkl.txt", fixed=TRUE));
## Do it and display the results:
#compute_BF(models.names);




#######################################################
#
# Useful functions
#
#######################################################


# Show model report:
print.model.report <- function(model.name, model.short.name, section=3)
{
  cat(paste0(paste0(rep("#",section),collapse="")," **",model.short.name,"**\n\n"));
  
  cat(paste0(paste0(rep("#",section+1),collapse="")," Convergence diagnostics\n\n"));
  
  tmp <- mcmc.diags.for.model(model.file.name=model.name, thin=10);
  
  cat(paste0("Model's formal name is: `",model.name,"`. It has **",tmp$runs, "** independent runs. \n\n"));
  
  if( !is.null(tmp$gelman) )
  {
    print(knitr::kable(tmp$gelman$psrf, digits=c(4, 4), 
                       caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables."));
    cat("\n");
    
    cat(paste0("The multivariate *psrf* is ", round(tmp$gelman$mpsrf,4), ".\n\n"));
  }

  #cat("\nThe manual investigation of the chains using `Tracer` suggests ...");
  
  #if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
  
  tmp.mlk <- load_marg_lkl(paste0(model.name,"_R1"));
  if( !is.null(tmp.mlk) )
  {
    #cat(paste0(paste0(rep("#",section+1),collapse="")," Marginal likelihood\n\n"));
    
    print(knitr::kable(tmp.mlk, digits=c(NA, 2), caption="Marginal likelihoods."));
  }
  cat("\n");
  
  if( !is.null(tmp$mcmc) )
  {
    cat(paste0(paste0(rep("#",section+1),collapse="")," Summaries and plots\n\n"));
    
    for( i in seq_along(tmp$mcmc) )
    {
      print(knitr::kable(summary(tmp$mcmc[[i]])$statistics, digits=2, caption=paste0("Summaries for chain *",i,"*.")));
    }
  }
  
  return (tmp);
}
```

## Intro

This document contains the results from running the various phylogenetic models contained in the `LD_models.Rev` `RevBayes` script which are a response to the phylogenetic aspects of Sergei Tarasov & Josef C. Uyeda's 2019 Technical Comment on our paper "Human sound systems are shaped by post-Neolithic changes in bite configuration" (henceforth denoted as "TU").

### Data

Just as T&U, we use a sample of the data from our original paper, consisting of two posterior tree samples + the MCC consensus tree for each (these samples are identified as "B" and "C"), for the 10 characters: `r char_names` (in some plots these appear as `r names(char_names)`).

### Tree sampling vs. given tree

In their original work, T&U use the tree as yet another parameter of the model, by including a uniformly distributed number between 1 and the number of trees in the sample (here, 1,000), `tr_id`, which changes using a `slide` move and which represents the index of the current tree, `psi`, in the trees sample.
While for the model used by T&U (denoted here as **T00**) this mechanism seems to work well (in the sense that `tr_id` samples well the whole range 1..1000), for other models it seems to become stuck at a single value (sometimes very early on).
Thus, I have tried an alternative model for `tr_id` (while T&U, in fact, use a "hidden" *real* uniform number `tr_rnd` which is rounded to produce `tr_id`, I have directly modelled `tr_id` as a uniformaly distributed *natural* number using `RevBayes`'s `dnUniformNatural` and `mvRandomIntegerWalk`) which produces similar results.
Therefore, leaving aside the reasons for this behaviour of the trees sample exploration for different models, I also decided to run trees from the sample *independently*, considering `psi` as *fixed* for a given run (but due to computational costs, I could un only a small subsample of the 1,000 trees).
Moreover, I also ran independently the two MCC consensus trees for the "B" and "C" samples.

### Implementation

I started from the original `.Rev` scripts provided by T&U, but I completely re-engineered them into a self-contained `RevBayes` script `LD_models.Rev` that implements, based on user-given parameters, a set of different a models (described below).
The script can either be run directly from `RevBayes` (for the whole tree sample) or from a set of provided `Bash` shell scripts (for each tree independently).
To make sure things converge, I let the process run for longer than T&U originally did (see details below).

I ran [`RevBayes`](https://revbayes.github.io/) version 1.0.12 downloaded and compiled on Linux from its [GitHub repository](https://github.com/revbayes/revbayes) as per the instructions [here](https://revbayes.github.io/software).
I used three Linux desktop computers for running these scripts:

  - Intel Core i7-4790K @4.00GHz 4 cores/4 threads (HT disabled) 32Gb RAM with Ubuntu 16.04,
  - Intel Core i7-3770 @3.40GHz 4 cores/4 threads (HT disabled) 16Gb RAM with Ubuntu 16.04,
  - AMD Ryzen 2700X @3.70GHz 8 cores/16 threads 32 Gb RAM with Ubuntu 18.04.

This `Rmarkdown` script contains all the analyses except, when specified, explorations using [`Tracer`](https://www.beast2.org/tracer-2/).

**Note:** to save disk space, we have compressed all `.log` and `_anc_states.txt` files using `XZ` (more precisely, with the `xz -9 *.log` and `xz -9 *_anc_states.txt` commands), and we read them in `R` using `xzfile(...)`.

### Models

For both "tree sampling" and "given tree" scenarios we implemented the same models:

| brm                | rfm             | srm              | name | description                              | comments                            | 
|:-------------------|:----------------|:-----------------|:-----|-----------------------------------------:|------------------------------------:|
| MC                 | 1               | 1                | M00  | simplest model                           |                                     |
| MC                 | DP              | MM               | M11  | full model - single branch rate          |                                     |
| ULR                | 1               | 1                | U00  | uncorrelated branch rate                 |                                     |
| ULR                | DP              | MM               | U11  | full model - uncorrelated branch rate    |                                     |
| RMC                | 1               | 1                | R00  | allow branch rate variation              |                                     |
| RMC                | DP              | 1                | R10  | allow branch rate variation + root prior |                                     |
| RMC                | 1               | MM               | R01  | allow branch and site rate variation     |                                     |
| RMC                | DP              | MM               | R11  | full model - branch rates                |                                     |
| TU                 | 1               | 1                | T00  | original TU model                        | cannot estimate marginal likelihood |
| TUBR               | 1               | 1                | B00  | original TU model + base branch rate     | --"--                               |
| TU                 | DP              | 1                | T10  | original TU model + root prior           | --"--                               |
| TU                 | 1               | MM               | T01  | original TU model + site rate variation  | --"--                               |

where:

  - **brm** (the **branch_rates_model**) describes how the branch lengths are modelled across the tree, and can be:
  
    + **TU**: the original T&U Dirichlet Process Prior model with an expected ~3 categories of rates across the tree (down from their original 5);
    + **TUBR**: intrestingly, while T&U were apparently inspired by a `RevBayes` tutorial ([m_DPP_bears.Revs](https://github.com/revbayes/revbayes_tutorial/blob/master/RB_DPPRelaxedClock_Tutorial/scripts/m_DPP_bears.Revs)), they left aside the tutorial's model of a base rate of change and instead used only the Dirichlet process prior for the branch rates; here, I re-introduced the original base rate with a lognormal prior, with the final branch rates being the product of the base rate the "raw" branch length produced by the Dirichlet process prior;
    + **MC**: is the simplest "molecular clock" model with a single rate (Gamma-distributed) across all branches, inspired from the `RevBayes` tutorial [Relaxed Clocks & Time Trees]( https://revbayes.github.io/tutorials/clocks/#specifying-branch-rate-models);
    + **ULR**: is the uncorrelated lognormal rates model, inspired from the `RevBayes` tutorial [Relaxed Clocks & Time Trees](https://revbayes.github.io/tutorials/clocks/#specifying-branch-rate-models);
    + **RMC**: this is a "relaxed morphological clock", inspired from the `RevBayes` tutorial [Relaxed Brownian Rate Estimation](https://revbayes.github.io/tutorials/cont_traits/relaxed_bm.html), which models the probability that a shift in rate occurs on a branch.

  - **rfm** (the **root_freq_model**) describes how the root values are estimated, and can be:
  
    + **1**: the original T&U constant `simplex(1,1)`, or
    + **DP**: the root state frequencies are not drawn from the stationary distribution, but from a Dirichlet prior distribution, adapted from the `RevBayes` tutorial [Discrete morphology - Ancestral State Estimation](https://revbayes.github.io/tutorials/morph/morph_more.html).
    
  - **srm** (the **site_rates_model**) describes how rates vary (or not) between sites (characters), and can be:
  
    + **1**: the original T&U single `Q` matrix across sites, or
    + **MM**: a mixture model with two categories of sites ("slow" and "fast") giverned by a `Q` matrix as for **1** (see below); this is inpsired from the `RevBayes` tutorial [Discrete morphology - Tree Inference](https://revbayes.github.io/tutorials/morph/)
    
The `Q`-matrix is the one originally modelled by T&U, namely:
$$
Q = 
\begin{bmatrix}
0 & 1 \\
r12 & 0 \\
\end{bmatrix}
$$

where r12 is either distributed exponentially (for **1**) or from a discretized Beta distribution (for **MM**).

We will use throughout the short **name** of the model (i.e., **M00** for the simplest molecular clock model).

#### Relevant model parameters

Here I describe the relevant parameters tracked by the MCMC process for each model (for model names, we use: the '?' represents any character; '^' means not the following character):

| parameter(s)                       | model(s) they apply to                    | interpretation                   | 
|:-----------------------------------|:------------------------------------------|---------------------------------:|
| `tr_id`                            | all (when using the whole trees sample)   | the index of the tree currently used from the sample |
| `Prior`, `Likelihood`, `Posterior` | all                                       | automatically tracked by `RevBayes`; as `ln()` |
| `mean_rt`                          | all                                       | the mean rate across branches and sites |
| `root_freq[1]`, `root_freq[2]`     | root freq models (**?1?**)                | the frequencies of the two states (0 or 1) at the root |
| `r12`                              | single-rate models (**??0**)              | the rate across sites as given by the single `Q` matrix |
| `cats_r12[1]`, `cats_r12[2]`       | mixture models (**??1**)                  | the two rates for the two categories of sites as given by the two `Q` matrices |
| `beta_cats_rates`                  | mixture models (**??1**)                  | the parameter of the discretized Beta distribution describing the two rate categories `cats_r12[1]` and `cats_r12[2]`  |
| `site_rates[1]`, `site_rates[2]`   | mixture models (**??1**)                  | the actual site rates |
| `alpha_site_rates`                 | mixture models (**??1**)                  | the parameter of the discretized Gamma distribution describing the two site rates `site_rates[1]` and `site_rates[2]`  |
| `n_cats`                           | all models except **MC** (**^M??**)       | the number of rate categories across branches  |
| `concentration`                    | **TU** and **TUBR** (**T??** and **B??**) | the concentration parameter of the Dirichlet Prior Process describing the expected number of rate categories across branches |
| `base_rate`                        | **TUBR** (**B??**)                        | the base rate multipler for all branches |
| `ucln_mu`, `ucln_sigma`            | **ULR** (**U??**)                         | parameters describing the log-normal distribution of the branch rates |
| `sigma2_root`                      | **RMC** (**R??**)                         | root branch rate parameter |



### Methodology

Please note that for the **TU** and **TUBR** models, the estimation of marginal likelihoods using `RevBayes`'s `powerPosterior.run()` function fails with an uninformative error message ("Cannot apply Gibbs sampler when the probability is heated."), which I traced back to their use of the *Dirichlet Prior Process* (`dnDPP`) and, more preciely, its associated moves (`mvDPPValueScaling`, `mvDPPAllocateAuxGibbs` and `mvDPPGibbsConcentration`); therefore, for these models I could not use formal model comparison with Bayes Factors (as described in `RevBayes`'s manuals) and instead will simply compare likelhoods.
For all other types of models I used Bayes Factors computed in this script.

For all "trees sample" models and for a subselection of the "individual trees" I ran 2 independent chains so I could ascertain the convergence (or lack thereof) of the MCMC process, using both **Gelman and Rubin's covergence diagnostics** (or *potential scale reduction factors*, shortened to *psrf*s, where convergence is indicated by values close to 1.0), and the visual inspection of the traces with `Tracer` (see, for example [here](https://beast.community/tracer_convergence), for details).


## Using the whole trees sample

Here we look at the results of the models that use the whole trees sample; there are two such samples: the "B" and the "C" trees. 
For these runs, we used the following `RevBayes` params:

| `RevBayes` param                       | meaning                                                             | value  | 
|:---------------------------------------|:--------------------------------------------------------------------|-------:|
| `burnin_generations`                   | number of generations for burn-in                                   | 10000  |
| `burnin_tuningInterval`                | tuning interval                                                     | 5000   |
| `mcmc_generations`                     | number of generations for the actual MCMC process                   | 500000 |
| `printgen_model`, `printgen_ancstates` | "thinning" interval for the full model and for the ancestral states | 10     |
| `marg_lkl_cats`                        | number of steps for the computation of marginal likelihood          | 50     |
| `marg_lkl_burnin_gen`                  | burn-in for the computation of marginal likelihood                  | 100000 |
| `marg_lkl_burnin_tun`                  | tuning interval for the computation of marginal likelihood          | 1000   |
| `marg_lkl_pow_gen`                     | number of generations for the computation of marginal likelihood    | 10000  |


```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Collect "Posterior", "Likelihood" and "Prior" for pseudo-model comparisons....
models.lkls <- NULL;
```

### **MC**

#### "B" trees

```{r M00 B all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=MC,rf=1,sr=1]_B(all+TU)"; model.short.name <- "M00";
m00 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="B", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r M11 B all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=MC,rf=DP,sr=MM]_B(all+TU)"; model.short.name <- "M11";
m11 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="B", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```

#### "C" trees

```{r M00 C all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=MC,rf=1,sr=1]_C(all+TU)"; model.short.name <- "M00";
m00 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="C", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r M11 C all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=MC,rf=DP,sr=MM]_C(all+TU)"; model.short.name <- "M11";
m11 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="C", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```

### **ULR**

#### "B" trees

```{r U00 B all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=ULR,rf=1,sr=1]_B(all+TU)"; model.short.name <- "U00";
u00 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="B", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r U11 B all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=ULR,rf=DP,sr=MM]_B(all+TU)"; model.short.name <- "U11";
u11 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="B", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```

#### "C" trees

```{r U00 C all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=ULR,rf=1,sr=1]_C(all+TU)"; model.short.name <- "U00";
u00 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="C", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r U11 C all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=ULR,rf=DP,sr=MM]_C(all+TU)"; model.short.name <- "U11";
u11 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="C", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


### **RMC**

#### "B" trees

```{r R00 B all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=RMC,rf=1,sr=1]_B(all+TU)"; model.short.name <- "R00";
r00 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="B", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r R10 B all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=RMC,rf=DP,sr=1]_B(all+TU)"; model.short.name <- "R10";
r10 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="B", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r R01 B all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=RMC,rf=1,sr=MM]_B(all+TU)"; model.short.name <- "R01";
r01 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="B", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r R11 B all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=RMC,rf=DP,sr=MM]_B(all+TU)"; model.short.name <- "R11";
r11 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="B", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```

#### "C" trees

```{r R00 C all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=RMC,rf=1,sr=1]_C(all+TU)"; model.short.name <- "R00";
r00 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="C", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r R10 C all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=RMC,rf=DP,sr=1]_C(all+TU)"; model.short.name <- "R10";
r10 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="C", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r R01 C all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=RMC,rf=1,sr=MM]_C(all+TU)"; model.short.name <- "R01";
r01 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="C", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r R11 C all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=RMC,rf=DP,sr=MM]_C(all+TU)"; model.short.name <- "R11";
r11 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="C", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


### **TU**

#### "B" trees

```{r T00 B all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=TU,rf=1,sr=1]_B(all+TU)"; model.short.name <- "T00";
t00 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="B", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r T10 B all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=TU,rf=DP,sr=1]_B(all+TU)"; model.short.name <- "T10";
t10 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="B", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r T01 B all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=TU,rf=1,sr=MM]_B(all+TU)"; model.short.name <- "T01";
t01 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="B", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```

#### "C" trees

```{r T00 C all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=TU,rf=1,sr=1]_C(all+TU)"; model.short.name <- "T00";
t00 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="C", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r T10 C all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=TU,rf=DP,sr=1]_C(all+TU)"; model.short.name <- "T10";
t10 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="C", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


```{r T01 C all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=TU,rf=1,sr=MM]_C(all+TU)"; model.short.name <- "T01";
t01 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="C", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


### **TUBR**

#### "B" trees

```{r B00 B all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=TUBR,rf=1,sr=1]_B(all+TU)"; model.short.name <- "B00";
b00 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="B", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```

#### "C" trees

```{r B00 C all, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
model.full.name <- "LD_[br=TUBR,rf=1,sr=1]_C(all+TU)"; model.short.name <- "B00";
b00 <- tmp <- print.model.report(model.full.name, model.short.name=model.short.name, section=5);
for( i in seq_along(tmp$mcmc) )
{
  models.lkls <- rbind(models.lkls,
                       data.frame("trees"="C", "model"=model.short.name, "run"=i, 
                                  "Posterior"=tmp$mcmc[[i]][,"Posterior"], "Likelihood"=tmp$mcmc[[i]][,"Likelihood"], "Prior"=tmp$mcmc[[i]][,"Prior"]));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ifelse(is.null(tmp$vars),3,length(tmp$vars))/3))}
if( !is.null(tmp$mcmc) ) plot(tmp$mcmc);
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=2*ceiling(10/3)}
root_anc_states(model.name=model.full.name, section=5);
```


### Model comparison

Here we use *Bayes Factors* to compare those models where we could estimate the marginal likelihood.

#### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
## List all models with marginal likelihood estimates;
#models.names <- unlist(strsplit(list.files(path="../output/pow_post/", pattern=glob2rx("*all*_R1_marg_lkl.txt"), full.names=FALSE, include.dirs=FALSE),
#                                "_marg_lkl.txt", fixed=TRUE));

# Models to compare (with full and short names):
models.to.compare <- data.frame("full.name"=c("LD_[br=MC,rf=1,sr=1]_B(all+TU)_R1", 
                                              "LD_[br=MC,rf=DP,sr=MM]_B(all+TU)_R1",
                                              "LD_[br=ULR,rf=1,sr=1]_B(all+TU)_R1",
                                              "LD_[br=ULR,rf=DP,sr=MM]_B(all+TU)_R1",
                                              "LD_[br=RMC,rf=1,sr=1]_B(all+TU)_R1",
                                              "LD_[br=RMC,rf=DP,sr=1]_B(all+TU)_R1",
                                              "LD_[br=RMC,rf=1,sr=MM]_B(all+TU)_R1",
                                              "LD_[br=RMC,rf=DP,sr=MM]_B(all+TU)_R1"), 
                                "short.name"=c("M00",
                                               "M11",
                                               "U00",
                                               "U11",
                                               "R00",
                                               "R10",
                                               "R01",
                                               "R11"), 
                                stringsAsFactors=FALSE);
# Do it and display the results:
df <- compute_BF(models.to.compare$full.name);
df$models$model <- vapply(df$models$model, function(s) models.to.compare$short.name[ models.to.compare$full.name == s], character(1));
df$BFs$M1 <- vapply(df$BFs$M1, function(s) models.to.compare$short.name[ models.to.compare$full.name == s], character(1));
df$BFs$M2 <- vapply(df$BFs$M2, function(s) models.to.compare$short.name[ models.to.compare$full.name == s], character(1));
df$BFs$supported <- vapply(df$BFs$supported, function(s) models.to.compare$short.name[ models.to.compare$full.name == s], character(1));

knitr::kable(df$models, digits=c(NA, NA, 2), row.names=FALSE, col.names=c("Model", "Method", "ln(mLkl)"),
             caption="The models for which the (log) marginal likelihood could be estimated.");
cat("\n");

knitr::kable(df$BFs, digits=c(NA, NA, NA, 2, 2, 2, NA, NA), row.names=FALSE, 
             col.names=c("M1", "M2", "Method", "ln(mLkl)M1", "ln(mLkl)M2", "ln(BF)M1-M2", "Supported", "Support"),
             caption="Pairwise model comparisons using (log) Bayes Factors.");
cat("\n");

ggplot(df$models[ order(df$models$model, df$models$Method), ], aes(x=model, y=Marginal_Likelihood, color=model, shape=Method)) + 
  geom_point(alpha=0.5, size=3) + geom_path(aes(group=Method), alpha=0.5) +
  ggtitle("The (log) marginal likelihood of the models") + 
  NULL;
```

#### "C" trees

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
## List all models with marginal likelihood estimates;
#models.names <- unlist(strsplit(list.files(path="../output/pow_post/", pattern=glob2rx("*all*_R1_marg_lkl.txt"), full.names=FALSE, include.dirs=FALSE),
#                                "_marg_lkl.txt", fixed=TRUE));

# Models to compare (with full and short names):
models.to.compare <- data.frame("full.name"=c("LD_[br=MC,rf=1,sr=1]_C(all+TU)_R1", 
                                              "LD_[br=MC,rf=DP,sr=MM]_C(all+TU)_R1",
                                              "LD_[br=ULR,rf=1,sr=1]_C(all+TU)_R1",
                                              "LD_[br=ULR,rf=DP,sr=MM]_C(all+TU)_R1",
                                              "LD_[br=RMC,rf=1,sr=1]_C(all+TU)_R1",
                                              "LD_[br=RMC,rf=DP,sr=1]_C(all+TU)_R1",
                                              "LD_[br=RMC,rf=1,sr=MM]_C(all+TU)_R1",
                                              "LD_[br=RMC,rf=DP,sr=MM]_C(all+TU)_R1"), 
                                "short.name"=c("M00",
                                               "M11",
                                               "U00",
                                               "U11",
                                               "R00",
                                               "R10",
                                               "R01",
                                               "R11"), 
                                stringsAsFactors=FALSE);
# Do it and display the results:
df <- compute_BF(models.to.compare$full.name);
df$models$model <- vapply(df$models$model, function(s) models.to.compare$short.name[ models.to.compare$full.name == s], character(1));
df$BFs$M1 <- vapply(df$BFs$M1, function(s) models.to.compare$short.name[ models.to.compare$full.name == s], character(1));
df$BFs$M2 <- vapply(df$BFs$M2, function(s) models.to.compare$short.name[ models.to.compare$full.name == s], character(1));
df$BFs$supported <- vapply(df$BFs$supported, function(s) models.to.compare$short.name[ models.to.compare$full.name == s], character(1));

knitr::kable(df$models, digits=c(NA, NA, 2), row.names=FALSE, col.names=c("Model", "Method", "ln(mLkl)"),
             caption="The models for which the (log) marginal likelihood could be estimated.");
cat("\n");

knitr::kable(df$BFs, digits=c(NA, NA, NA, 2, 2, 2, NA, NA), row.names=FALSE, 
             col.names=c("M1", "M2", "Method", "ln(mLkl)M1", "ln(mLkl)M2", "ln(BF)M1-M2", "Supported", "Support"),
             caption="Pairwise model comparisons using (log) Bayes Factors.");
cat("\n");

ggplot(df$models[ order(df$models$model, df$models$Method), ], aes(x=model, y=Marginal_Likelihood, color=model, shape=Method)) + 
  geom_point(alpha=0.5, size=3) + geom_path(aes(group=Method), alpha=0.5) +
  ggtitle("The (log) marginal likelihood of the models") + 
  NULL;
```


### (Pseudo) model comparison

Here we also consider the **TU** and **TUBR** models for which the marginal likelihood cannot be estimates, by using the actual *likelihoods* (but this far from optimal)...

```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
names(models.lkls) <- c("trees", "model", "run", "Posterior", "Likelihood", "Prior");
```

#### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=10, fig.height=5}
d <- models.lkls[ models.lkls$trees == "B", ];
d$run <- as.factor(d$run);

ggplot(d, aes(x=run, y=Likelihood, color=run)) + 
  geom_boxplot() + 
  facet_grid(. ~ model) +
  ggtitle("Boxplots of the models' (log) likelihood") + 
  NULL;

ggplot(d, aes(x=run, y=Likelihood, color=run)) + 
  geom_violin() + 
  facet_grid(. ~ model) +
  ggtitle("Violin plots of the models' (log) likelihood") + 
  NULL;

ggplot(d, aes(x=run, y=Prior, color=run)) + 
  geom_boxplot() + 
  facet_grid(. ~ model) +
  ggtitle("Boxplots of the models' prior") + 
  NULL;

ggplot(d, aes(x=run, y=Prior, color=run)) + 
  geom_violin() + 
  facet_grid(. ~ model) +
  ggtitle("Violin plots of the models' prior") + 
  NULL;

ggplot(d, aes(x=run, y=Posterior, color=run)) + 
  geom_boxplot() + 
  facet_grid(. ~ model) +
  ggtitle("Boxplots of the models' posterior") + 
  NULL;

ggplot(d, aes(x=run, y=Posterior, color=run)) + 
  geom_violin() + 
  facet_grid(. ~ model) +
  ggtitle("Violin plots of the models' posterior") + 
  NULL;
```

#### "C" trees

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=10, fig.height=5}
d <- models.lkls[ models.lkls$trees == "C", ];
d$run <- as.factor(d$run);

ggplot(d, aes(x=run, y=Likelihood, color=run)) + 
  geom_boxplot() + 
  facet_grid(. ~ model) +
  ggtitle("Boxplots of the models' (log) likelihood") + 
  NULL;

ggplot(d, aes(x=run, y=Likelihood, color=run)) + 
  geom_violin() + 
  facet_grid(. ~ model) +
  ggtitle("Violin plots of the models' (log) likelihood") + 
  NULL;

ggplot(d, aes(x=run, y=Prior, color=run)) + 
  geom_boxplot() + 
  facet_grid(. ~ model) +
  ggtitle("Boxplots of the models' prior") + 
  NULL;

ggplot(d, aes(x=run, y=Prior, color=run)) + 
  geom_violin() + 
  facet_grid(. ~ model) +
  ggtitle("Violin plots of the models' prior") + 
  NULL;

ggplot(d, aes(x=run, y=Posterior, color=run)) + 
  geom_boxplot() + 
  facet_grid(. ~ model) +
  ggtitle("Boxplots of the models' posterior") + 
  NULL;

ggplot(d, aes(x=run, y=Posterior, color=run)) + 
  geom_violin() + 
  facet_grid(. ~ model) +
  ggtitle("Violin plots of the models' posterior") + 
  NULL;
```


### Summary

#### "B" trees

Individual models: 

  - **M00**: convergence ok, similar params, good mixing, good tree sampling, chains are settling, ancestral states ~10% "1", `r12` ~ 0.7
  - **M11**: convergence not very good (but not very bad either), similar params, good mixing, good tree sampling in the begining then getting stuck on a single (different) tree, chains are settling, ancestral states ~20% "1", `alpha_site_rates` very large (~600000), `beta_cats_rates` ~ 0.20, `cats_r12` one very low one very high
  - **U00**: bad convergence, but similar params, tree sampling quickly gets stuck to different trees, ancestral states ~50% "1", `r12` ~ 2.20, `n_cats` ~ 102, `ucln_mu` ~ -4, `ucln_sigma` ~ 1.4
  - **U11**: bad convergence, relatively similar params, tree sampling quickly gets stuck to different trees, ancestral states ~50% "1", `alpha_site_rates` large (~80000), `beta_cats_rates` ~ 0.05, `cats_r12` one very low one very high, `n_cats` ~ 102, `ucln_mu` ~ -4, `ucln_sigma` ~ 1 or 2
  - **R00**: very bad convergence, but very similar params, bad mixing, tree is stuck from the begining (no exploration), ancestral states ~50% "1", `r12` ~ 2, `n_cats` ~ 12-13
  - **R10**: very bad convergence, but relatively similar params, bad mixing, tree is stuck from the begining (no exploration), ancestral states ~5% "1", `r12` ~ 0.5-1.5, `n_cats` ~ 6 or 17
  - **R01**: very bad convergence, different params, bad mixing, tree is stuck from the begining (no exploration), ancestral states ~5% "1", `alpha_site_rates` very large and different, `beta_cats_rates` small and different, `cats_r12` different between runs, `n_cats` ~ 12 or 15
  - **R11**: bad convergence, different params, bad mixing, tree is stuck from the begining (no exploration), ancestral states ~50% "1", `alpha_site_rates` very large and different, `beta_cats_rates` small and different, `cats_r12` different between runs, `n_cats` ~ 9 or 11
  - **T00**: convergence perfect, very similar params, very good mixing, very good tree sampling, chains are settling, ancestral states ~50% "1", `r12` ~ 2.4, `n_cats` = 1, `concentration` ~ 0.21 
  - **T10**: convergence perfect, very similar params, very good mixing, very good tree sampling, chains are settling, ancestral states ~50% "1", `r12` ~ 2.4, `n_cats` = 1, `concentration` ~ 0.21 
  - **T01**: good convergence, relatively similar params, good mixing, very good tree sampling, chains seem to settle, ancestral states ~25% "1", `alpha_site_rates` very large and different, `beta_cats_rates` small and different, `cats_r12` one very low one very high, `n_cats` ~ 1, `concentration` ~ 0.21 
  - **B00**: very good convergence, very similar params, very good mixing, relatively good tree sampling, chains are settling, ancestral states ~0% "1", `r12` ~ 0.2, `n_cats` = 2, `concentration` ~ 0.3, `base_rate` ~ 0.01 

Model (pseudo-)comparison: 

  - **T00**, **T10**, **T01** and **M00** are the *worst*, with **B00** *slightly better*, and **M11**, **R00**, **R01**, **R10**, **R11**, **U00** and **U11** being *better*.

#### "C" trees

Individual models: 

  - **M00**: convergence relatively ok, relatively similar params, relatively good mixing, relatively good tree sampling, chains seem to settle, ancestral states ~25% "1", `r12` ~ 0.6 or 1.4
  - **M11**: convergence ok, relatively similar params, relatively good mixing, relatively good tree sampling, chains are settling, ancestral states ~0% "1", `alpha_site_rates` very large (~450000), `beta_cats_rates` ~ 2.0, `cats_r12` one low (0.25) one high (0.75)
  - **U00**: super bad convergence, different params, tree sampling is stuck from the begining, ancestral states ~50% "1", `r12` ~ 0.05 or 2.16, `n_cats` ~ 102, `ucln_mu` ~ -3.13 or -0.10, `ucln_sigma` ~ 0.2 or 0.05
  - **U11**: bad convergence, relatively similar params, tree sampling quickly gets stuck to different trees, ancestral states ~50% "1", `alpha_site_rates` large (~300000), `beta_cats_rates` ~ 0.03, `cats_r12` one very low one very high, `n_cats` ~ 102, `ucln_mu` ~ -0.6 or -1, `ucln_sigma` ~ 0.7
  - **R00**: bad convergence, relatively similar params, bad mixing, tree is stuck from the begining (no exploration), ancestral states ~50% "1", `r12` ~ 0.05, `n_cats` ~ 17 or 25
  - **R10**: very bad convergence, relatively similar params, bad mixing, tree is stuck from the begining (no exploration), ancestral states ~50% "1", `r12` ~ 0.2, `n_cats` ~ 2 or 10
  - **R01**: very bad convergence, different params, bad mixing, tree is stuck from the begining (no exploration), ancestral states ~50% "1", `alpha_site_rates` very large and different, `beta_cats_rates` small and different, `cats_r12` different between runs, `n_cats` ~ 4 or 7
  - **R11**: very bad convergence, different params, bad mixing, tree is stuck from the begining (no exploration), ancestral states ~0% "1", `alpha_site_rates` very large and different, `beta_cats_rates` small and different, `cats_r12` different between runs, `n_cats` ~ 2 or 6
  - **T00**: convergence perfect, very similar params, very good mixing, very good tree sampling, chains are settling, ancestral states ~50% "1", `r12` ~ 2.2, `n_cats` = 1, `concentration` ~ 0.21 
  - **T10**: convergence perfect, very similar params, very good mixing, very good tree sampling, chains are settling, ancestral states ~50% "1", `r12` ~ 2.3, `n_cats` = 1, `concentration` ~ 0.21 
  - **T01**: convergence perfect, very similar params, very good mixing, very good tree sampling, chains seem to settle, ancestral states ~50% "1", `alpha_site_rates` ~ 500000, `beta_cats_rates` ~ 0.03, `cats_r12` one very low (0.02) one very high (0.98), `n_cats` = 1, `concentration` ~ 0.21 
  - **B00**: decent convergence, similar params, very good mixing, relatively good tree sampling, chains are settling, ancestral states ~0% "1", `r12` ~ 0.3-0.4, `n_cats` = 2, `concentration` ~ 0.32, `base_rate` ~ 0.01 

Model (pseudo-)comparison: 

  - **T00**, **T10** and **T01** are the *worst*, with **B00**, **M00** and **M11** *slightly better*, and **R00**, **R01**, **R10**, **U00**, **U11** and (possibly) **R11** being *better*.

### Interpretation

#### Tree sampling mechanism

The tree sampling mechanism implemented by T&U works well only in some models (particularly **TU**, but also relatively ok for **TUBR** and **MC**), while for others (**RMC** and **ULR**) it basically degenerates in a single-tree model per run (with no signs that other trees might be considered later on). 
Therefore, we should really consider that only **TU**, **TUBR** and **MC** are saying anything about the whole sample. 
Moreover, the apparently (very) poor convergence between the two runs for the **RMC** and **ULR** models is, in fact, an artifact of the tree sampling becoming stuck at different trees in the two runs, and does not say anything about the convergence of the MCMC processes itself.

An interesting questions concerns the meaning of this behaviour: one possible interpretation is that the parameters of the **TU**, **TUBR** and **MC** fit relativelly similarly all the trees in the sample, allowing the MCMC process to jump from tree to tree without much penalty, but the other models need different enough parameter values for different trees so that after they become optimised for a particular tree the penalty for trying a different tree is too big.

This interpretation is supported by the fact that **TU**, **TUBR** and **MC** are the worst fits for the data (see below).

#### Adding base rate back to the original T&U model

It is interesting to note that adding back the base rate to the original T&U model (as in the original `RevBayes` tutorial from which this was inspired) simultaneously:

  - *improves the fit* (on average, from about ~ -290 for **T00** to ~ -190 for **B00** for 1 extra numeric parameter, corresponding to a $\Delta AIC \approx (2k - 2 \times (-290)) - (2(k+1) - 2 \times (-190))$ = $2 \times 290 -2 - 2 \times 190$ = $198$ AIC points),
  - results in *ancestral state estimates at the root* of overwhelming absence of labiodentals (~0% reconstructed presence; in line with the original paper) from the argubaly wrong 50% for the original T&U model,
  - *increases `n_cats`* from 1 to 2, more in line with our original results.

Thus, unless theoretically strongly motivated, excluding the base rate (as done by the original T&U model) strongly distorts the results.

#### Adding root frequency and between-site rate variation to the original T&U model

These do not seem to dramatically change the ancestral state reconstructions, the fit to the data or the `n_cats` value.

#### Other models (**ULR**, **MC** and **RMC**)

As discussed above, the tree sampling mechanism does not seem to work as intended here, but, while the molecular clock **MC** is a bad fit to the data (comparable to the original **TU** and the base rate **TUBR**), the other two seem to be clear improvements.
It should be noted that the bad fit of **MC** should not come as a surprise, as it goes against everything we know about how sound change proceeds historically!



```{r stuff for single trees, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model short name can be either of the form "M00" or "M00mcc":
load.tree.results.for.model <- function(model.short.name, trees.sample="B", xz.compressed=TRUE)
{
  # Map the short name to the corresponding regular expression:
  name.reg.exp <- switch(model.short.name,
                         "M00"=paste0("LD_\\[br=MC,rf=1,sr=1\\]_",trees.sample,"\\([[:digit:]]+\\)"),
                         "M11"=paste0("LD_\\[br=MC,rf=DP,sr=MM\\]_",trees.sample,"\\([[:digit:]]+\\)"),
                         "R00"=paste0("LD_\\[br=RMC,rf=1,sr=1\\]_",trees.sample,"\\([[:digit:]]+\\)"),
                         "R11"=paste0("LD_\\[br=RMC,rf=DP,sr=MM\\]_",trees.sample,"\\([[:digit:]]+\\)"),
                         "T00"=paste0("LD_\\[br=TU,rf=1,sr=1\\]_",trees.sample,"\\([[:digit:]]+\\)"),
                         "B00"=paste0("LD_\\[br=TUBR,rf=1,sr=1\\]_",trees.sample,"\\([[:digit:]]+\\)"),
                         "M00mcc"=paste0("LD_\\[br=MC,rf=1,sr=1\\]_",trees.sample,"\\(MCC\\)"),
                         "M11mcc"=paste0("LD_\\[br=MC,rf=DP,sr=MM\\]_",trees.sample,"\\(MCC\\)"),
                         "R00mcc"=paste0("LD_\\[br=RMC,rf=1,sr=1\\]_",trees.sample,"\\(MCC\\)"),
                         "R11mcc"=paste0("LD_\\[br=RMC,rf=DP,sr=MM\\]_",trees.sample,"\\(MCC\\)"),
                         "T00mcc"=paste0("LD_\\[br=TU,rf=1,sr=1\\]_",trees.sample,"\\(MCC\\)"),
                         "B00mcc"=paste0("LD_\\[br=TUBR,rf=1,sr=1\\]_",trees.sample,"\\(MCC\\)"),
                         NA);
  if( is.na(name.reg.exp) ) return (NULL);
  
  # Map the short name to the corresponding variables to keep:
  vars.to.keep <- switch(model.short.name,
                         "M00mcc"=,
                         "M00"=c("Posterior", "Likelihood", "Prior", "mean_rt", "r12"),
                         "M11mcc"=,
                         "M11"=c("Posterior", "Likelihood", "Prior", "mean_rt", "root_freq[1]", "root_freq[2]", "cats_r12[1]", "cats_r12[2]", "site_rates[1]", "site_rates[2]", "beta_cats_rates", "alpha_site_rates"),
                         "R00mcc"=,
                         "R00"=c("Posterior", "Likelihood", "Prior", "mean_rt", "r12", "n_cats", "sigma2_root"),
                         "R11mcc"=,
                         "R11"=c("Posterior", "Likelihood", "Prior", "mean_rt", "root_freq[1]", "root_freq[2]", "cats_r12[1]", "cats_r12[2]", "site_rates[1]", "site_rates[2]", "beta_cats_rates", "alpha_site_rates", "n_cats", "sigma2_root"),
                         "T00mcc"=,
                         "T00"=c("Posterior", "Likelihood", "Prior", "mean_rt", "r12", "n_cats", "concentration"),
                         "B00mcc"=,
                         "B00"=c("Posterior", "Likelihood", "Prior", "mean_rt", "r12", "n_cats", "concentration", "base_rate"),
                         NULL);
  if( is.null(vars.to.keep) ) return (NULL);
  
  # Load all the individual logs:
  model.file.names <- list.files(path="../output/", pattern=glob2rx(paste0("*.log",ifelse(xz.compressed,".xz",""))), full.names=FALSE, include.dirs=FALSE);
  model.file.names <- model.file.names[ grep(name.reg.exp, model.file.names, fixed=FALSE) ];
  if( length(model.file.names) > 0 )
  {
    models.data <- cbind("Model"=model.short.name,
                         do.call(rbind, lapply(model.file.names, function(s) 
                         {
                           # Load the data:
                           if( xz.compressed )
                           {
                             d <- read.table(xzfile(paste0("../output/",s)), header=TRUE, sep="\t", quote="", check.names=FALSE)[,vars.to.keep];
                           } else
                           {
                             d <- read.table(paste0("../output/",s), header=TRUE, sep="\t", quote="", check.names=FALSE)[,vars.to.keep];
                           }
                           
                           # Extract the info from the file name:
                           tree <- strsplit(strsplit(s, "(", fixed=TRUE)[[1]][2], ")", fixed=TRUE)[[1]][1];
                           run  <- strsplit(strsplit(s, "_R", fixed=TRUE)[[1]][2], ".", fixed=TRUE)[[1]][1];
                           
                           # Return the data:
                           return (cbind("Tree"=tree, "Run"=run, d));
                         })));
    d <- melt(models.data, id.vars=c("Model", "Tree", "Run"), variable.name="Var", value.name="Val");
    models.summaries <- d %>% 
      group_by(Model, Tree, Run, Var) %>% 
      summarise("min"=min(Val,na.rm=TRUE), "max"=max(Val,na.rm=TRUE),
                "mean"=mean(Val,na.rm=TRUE), "median"=median(Val,na.rm=TRUE), 
                "sd"=sd(Val,na.rm=TRUE), "iqr"=IQR(Val,na.rm=TRUE));
    models.summaries.all <- d %>% 
      group_by(Model, Var) %>% 
      summarise("min"=min(Val,na.rm=TRUE), "max"=max(Val,na.rm=TRUE),
                "mean"=mean(Val,na.rm=TRUE), "median"=median(Val,na.rm=TRUE), 
                "sd"=sd(Val,na.rm=TRUE), "iqr"=IQR(Val,na.rm=TRUE));
  } else
  {
    models.data <- NULL;
  }
  
  # Load the marginal likelihoods:
  model.file.names <- unlist(strsplit(list.files(path="../output/pow_post/", pattern=glob2rx("*_marg_lkl.txt"), full.names=FALSE, include.dirs=FALSE),
                                      "_marg_lkl.txt", fixed=TRUE));
  model.file.names <- model.file.names[ grep(name.reg.exp, model.file.names, fixed=FALSE) ];
  if( length(model.file.names) > 0 )
  {
    models.mlkl <- cbind("Model"=model.short.name,
                         do.call(rbind, lapply(model.file.names, function(s) 
                         {
                           # Load the data:
                           d <- load_marg_lkl(s);
                           
                           # Extract the info from the file name:
                           tree <- strsplit(strsplit(s, "(", fixed=TRUE)[[1]][2], ")", fixed=TRUE)[[1]][1];
                           run  <- strsplit(strsplit(s, "_R", fixed=TRUE)[[1]][2], ".", fixed=TRUE)[[1]][1];
                           
                           # Return the data:
                           return (cbind("Tree"=ifelse(!is.na(as.numeric(tree)), as.numeric(tree), tree), "Run"=as.numeric(run), d));
                         })));
    models.mlkl <- models.mlkl[ order(models.mlkl$Model, models.mlkl$Tree, models.mlkl$Run), ]
  } else
  {
    models.mlkl <- NULL;
  }
  
  # Load the ancestral states at the root:
  model.file.names <- list.files(path="../output/anc_states/", pattern=glob2rx("*_anc_states.txt.xz"), full.names=FALSE, include.dirs=FALSE);
  model.file.names <- model.file.names[ grep(name.reg.exp, model.file.names, fixed=FALSE) ];
  if( length(model.file.names) > 0 )
  {
    models.ancsts <- data.frame("Model"=model.short.name,
                           do.call(rbind, lapply(model.file.names, function(s) 
                           {
                             # Load the data:
                             if( xz.compressed )
                             {
                               d <- read.table(paste0("../output/anc_states/",s), header=TRUE, sep="\t", quote="");
                             } else
                             {
                               d <- read.table(xzfile(paste0("../output/anc_states/",s)), header=TRUE, sep="\t", quote="");
                             }
                             
                             # Keep only the first (Iteration) and last column (root states):
                             d <- d[,c(1,ncol(d))];
                             
                             # Split the last column into indepent characters:
                             d <- as.data.frame(cbind(d$Iteration, do.call(rbind, strsplit(as.character(d[,2]), ",", fixed=TRUE))));
                             names(d) <- c("iteration", names(char_names));
                             for( i in 1:ncol(d) ) d[,i] <- as.numeric(as.character(d[,i])); # convert them to numeric
                             
                             # Summarize them:
                             d.sum <- colSums(d[,-1]) / nrow(d);
                             
                             # Extract the info from the file name:
                             tree <- strsplit(strsplit(s, "(", fixed=TRUE)[[1]][2], ")", fixed=TRUE)[[1]][1];
                             run  <- strsplit(strsplit(s, "_R", fixed=TRUE)[[1]][2], "_anc", fixed=TRUE)[[1]][1];
                             
                             # Return the data:
                             return (c("Tree"=ifelse(!is.na(as.numeric(tree)), as.numeric(tree), tree), "Run"=as.numeric(run), d.sum));
                           })));
    models.ancsts <- models.ancsts[ order(models.ancsts$Model, models.ancsts$Tree, models.ancsts$Run), ]; # proportion of "1" values at the root per tree and run
    # Get the trees as well (if they exist):
    models.anctrees <- model.file.names <- list.files(path="../output/anc_states/", pattern=glob2rx("*_chr_*.tree"), full.names=FALSE, include.dirs=FALSE);
    models.anctrees <- models.anctrees[ grep(name.reg.exp, models.anctrees, fixed=FALSE) ];
  } else
  {
    models.ancsts <- NULL;
    models.anctrees <- NULL;
  }
  
  # Return value:
  return (list("posterior.vars"=models.data, 
               "summaries.vars"=list("by.tree"=models.summaries, "overall"=models.summaries.all), 
               "prop.1.at.root"=models.ancsts, "anc.sts.trees"=models.anctrees,
               "marg.likelihoods"=models.mlkl));
}
```


## The MCC consensus trees

Here we look at the results of the models applied to the MCC consensus trees for the "B" and the "C" samples independenlty.
We apply similar methods to those used for the independent individual trees and described in detail there.

For these runs, we used the following `RevBayes` params:

| `RevBayes` param                       | meaning                                                             | value   | 
|:---------------------------------------|:--------------------------------------------------------------------|--------:|
| `burnin_generations`                   | number of generations for burn-in                                   | 100000  |
| `burnin_tuningInterval`                | tuning interval                                                     | 10000   |
| `mcmc_generations`                     | number of generations for the actual MCMC process                   | 1000000 |
| `printgen_model`, `printgen_ancstates` | "thinning" interval for the full model and for the ancestral states | 100     |
| `marg_lkl_cats`                        | number of steps for the computation of marginal likelihood          | 25      |
| `marg_lkl_burnin_gen`                  | burn-in for the computation of marginal likelihood                  | 100000  |
| `marg_lkl_burnin_tun`                  | tuning interval for the computation of marginal likelihood          | 10000   |
| `marg_lkl_pow_gen`                     | number of generations for the computation of marginal likelihood    | 50000   |


### **MC**

#### **M00**

##### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
m00.mcc.B <- tmp <- load.tree.results.for.model("M00mcc", trees.sample="B");
```

Convergence diagnostics and plots:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Convergence diagnostics:
run_mcmc <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ], thin=100)));
# Check and remove constant columns (they mess up gelman.diag):
const.cols <- colSums(do.call(rbind, lapply(unique(tmp$posterior.vars$Run), function(r){ x <- tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
# Transform them into MCMC objects (if the case):
if( sum(const.cols == 0) > 0 )
{
  # For glemann, keep only the non-constant variables:
  run_mcmc_g <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -union(1:3, 3+which(const.cols > 0)) ], thin=100)));
  
  # Compute and print the results:
  gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
  
  print(knitr::kable(gdiag$psrf, digits=c(4, 4), 
                     caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables."));
  cat("\n");
  
  cat(paste0("The multivariate *psrf* is ", round(gdiag$mpsrf,4), ".\n\n"));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ncol(run_mcmc[[1]])/3))}
if( !is.null(run_mcmc) ) plot(run_mcmc);
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across runs") +
  facet_grid(Variable ~ ., scales="free_y") + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
for( i in seq_along(run_mcmc) )
{
  print(knitr::kable(summary(run_mcmc[[i]])$statistics, digits=2, caption=paste0("Summaries for chain *",i,"*.")));
}
#knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  knitr::kable(as.data.frame(d[,c("Method", "Marginal_Likelihood")]), digits=c(NA, 2), row.names=FALSE, caption="Marginal likelihoods.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4*5, fig.cap="Reconstructed ancestral states throughout the tree (maximum posterior probability states or 'MAP')."}
# Ancestral trees:
plot_anc_states_trees(paste0("../output/anc_states/",tmp$anc.sts.trees));
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Percent of values reconstructed as '1' at the root."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- as.numeric(d$Perc.1.val) * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_point() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(d[,c("Character", "Perc.1.val")]), digits=c(NA, 2), row.names=FALSE, caption="Probability (as percent, %) to reconstruct a '1' at the root for each character.");
```


##### "C" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
m00.mcc.C <- tmp <- load.tree.results.for.model("M00mcc", trees.sample="C");
```

Convergence diagnostics and plots:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Convergence diagnostics:
run_mcmc <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ], thin=100)));
# Check and remove constant columns (they mess up gelman.diag):
const.cols <- colSums(do.call(rbind, lapply(unique(tmp$posterior.vars$Run), function(r){ x <- tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
# Transform them into MCMC objects (if the case):
if( sum(const.cols == 0) > 0 )
{
  # For glemann, keep only the non-constant variables:
  run_mcmc_g <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -union(1:3, 3+which(const.cols > 0)) ], thin=100)));
  
  # Compute and print the results:
  gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
  
  print(knitr::kable(gdiag$psrf, digits=c(4, 4), 
                     caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables."));
  cat("\n");
  
  cat(paste0("The multivariate *psrf* is ", round(gdiag$mpsrf,4), ".\n\n"));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ncol(run_mcmc[[1]])/3))}
if( !is.null(run_mcmc) ) plot(run_mcmc);
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across runs") +
  facet_grid(Variable ~ ., scales="free_y") + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
for( i in seq_along(run_mcmc) )
{
  print(knitr::kable(summary(run_mcmc[[i]])$statistics, digits=2, caption=paste0("Summaries for chain *",i,"*.")));
}
#knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  knitr::kable(as.data.frame(d[,c("Method", "Marginal_Likelihood")]), digits=c(NA, 2), row.names=FALSE, caption="Marginal likelihoods.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4*5, fig.cap="Reconstructed ancestral states throughout the tree (maximum posterior probability states or 'MAP')."}
# Ancestral trees:
plot_anc_states_trees(paste0("../output/anc_states/",tmp$anc.sts.trees));
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Percent of values reconstructed as '1' at the root."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- as.numeric(d$Perc.1.val) * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_point() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(d[,c("Character", "Perc.1.val")]), digits=c(NA, 2), row.names=FALSE, caption="Probability (as percent, %) to reconstruct a '1' at the root for each character.");
```


#### **M11**

##### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
m11.mcc.B <- tmp <- load.tree.results.for.model("M11mcc", trees.sample="B");
```

Convergence diagnostics and plots:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Convergence diagnostics:
run_mcmc <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ], thin=100)));
# Check and remove constant columns (they mess up gelman.diag):
const.cols <- colSums(do.call(rbind, lapply(unique(tmp$posterior.vars$Run), function(r){ x <- tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
# Transform them into MCMC objects (if the case):
if( sum(const.cols == 0) > 0 )
{
  # For glemann, keep only the non-constant variables:
  run_mcmc_g <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -union(1:3, 3+which(const.cols > 0)) ], thin=100)));
  
  # Compute and print the results:
  gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
  
  print(knitr::kable(gdiag$psrf, digits=c(4, 4), 
                     caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables."));
  cat("\n");
  
  cat(paste0("The multivariate *psrf* is ", round(gdiag$mpsrf,4), ".\n\n"));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ncol(run_mcmc[[1]])/3))}
if( !is.null(run_mcmc) ) plot(run_mcmc);
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across runs") +
  facet_grid(Variable ~ ., scales="free_y") + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
for( i in seq_along(run_mcmc) )
{
  print(knitr::kable(summary(run_mcmc[[i]])$statistics, digits=2, caption=paste0("Summaries for chain *",i,"*.")));
}
#knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  knitr::kable(as.data.frame(d[,c("Method", "Marginal_Likelihood")]), digits=c(NA, 2), row.names=FALSE, caption="Marginal likelihoods.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4*5, fig.cap="Reconstructed ancestral states throughout the tree (maximum posterior probability states or 'MAP')."}
# Ancestral trees:
plot_anc_states_trees(paste0("../output/anc_states/",tmp$anc.sts.trees));
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Percent of values reconstructed as '1' at the root."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- as.numeric(d$Perc.1.val) * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_point() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(d[,c("Character", "Perc.1.val")]), digits=c(NA, 2), row.names=FALSE, caption="Probability (as percent, %) to reconstruct a '1' at the root for each character.");
```

##### "C" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
m11.mcc.C <- tmp <- load.tree.results.for.model("M11mcc", trees.sample="C");
```

Convergence diagnostics and plots:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Convergence diagnostics:
run_mcmc <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ], thin=100)));
# Check and remove constant columns (they mess up gelman.diag):
const.cols <- colSums(do.call(rbind, lapply(unique(tmp$posterior.vars$Run), function(r){ x <- tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
# Transform them into MCMC objects (if the case):
if( sum(const.cols == 0) > 0 )
{
  # For glemann, keep only the non-constant variables:
  run_mcmc_g <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -union(1:3, 3+which(const.cols > 0)) ], thin=100)));
  
  # Compute and print the results:
  gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
  
  print(knitr::kable(gdiag$psrf, digits=c(4, 4), 
                     caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables."));
  cat("\n");
  
  cat(paste0("The multivariate *psrf* is ", round(gdiag$mpsrf,4), ".\n\n"));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ncol(run_mcmc[[1]])/3))}
if( !is.null(run_mcmc) ) plot(run_mcmc);
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across runs") +
  facet_grid(Variable ~ ., scales="free_y") + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
for( i in seq_along(run_mcmc) )
{
  print(knitr::kable(summary(run_mcmc[[i]])$statistics, digits=2, caption=paste0("Summaries for chain *",i,"*.")));
}
#knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  knitr::kable(as.data.frame(d[,c("Method", "Marginal_Likelihood")]), digits=c(NA, 2), row.names=FALSE, caption="Marginal likelihoods.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4*5, fig.cap="Reconstructed ancestral states throughout the tree (maximum posterior probability states or 'MAP')."}
# Ancestral trees:
plot_anc_states_trees(paste0("../output/anc_states/",tmp$anc.sts.trees));
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Percent of values reconstructed as '1' at the root."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- as.numeric(d$Perc.1.val) * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_point() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(d[,c("Character", "Perc.1.val")]), digits=c(NA, 2), row.names=FALSE, caption="Probability (as percent, %) to reconstruct a '1' at the root for each character.");
```


### **RMC**

#### **R00**

##### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
r00.mcc.B <- tmp <- load.tree.results.for.model("R00mcc", trees.sample="B");
```

Convergence diagnostics and plots:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Convergence diagnostics:
run_mcmc <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ], thin=100)));
# Check and remove constant columns (they mess up gelman.diag):
const.cols <- colSums(do.call(rbind, lapply(unique(tmp$posterior.vars$Run), function(r){ x <- tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
# Transform them into MCMC objects (if the case):
if( sum(const.cols == 0) > 0 )
{
  # For glemann, keep only the non-constant variables:
  run_mcmc_g <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -union(1:3, 3+which(const.cols > 0)) ], thin=100)));
  
  # Compute and print the results:
  gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
  
  print(knitr::kable(gdiag$psrf, digits=c(4, 4), 
                     caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables."));
  cat("\n");
  
  cat(paste0("The multivariate *psrf* is ", round(gdiag$mpsrf,4), ".\n\n"));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ncol(run_mcmc[[1]])/3))}
if( !is.null(run_mcmc) ) plot(run_mcmc);
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across runs") +
  facet_grid(Variable ~ ., scales="free_y") + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
for( i in seq_along(run_mcmc) )
{
  print(knitr::kable(summary(run_mcmc[[i]])$statistics, digits=2, caption=paste0("Summaries for chain *",i,"*.")));
}
#knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  knitr::kable(as.data.frame(d[,c("Method", "Marginal_Likelihood")]), digits=c(NA, 2), row.names=FALSE, caption="Marginal likelihoods.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4*5, fig.cap="Reconstructed ancestral states throughout the tree (maximum posterior probability states or 'MAP')."}
# Ancestral trees:
plot_anc_states_trees(paste0("../output/anc_states/",tmp$anc.sts.trees));
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Percent of values reconstructed as '1' at the root."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- as.numeric(d$Perc.1.val) * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_point() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(d[,c("Character", "Perc.1.val")]), digits=c(NA, 2), row.names=FALSE, caption="Probability (as percent, %) to reconstruct a '1' at the root for each character.");
```

##### "C" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
r00.mcc.C <- tmp <- load.tree.results.for.model("R00mcc", trees.sample="C");
```

Convergence diagnostics and plots:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Convergence diagnostics:
run_mcmc <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ], thin=100)));
# Check and remove constant columns (they mess up gelman.diag):
const.cols <- colSums(do.call(rbind, lapply(unique(tmp$posterior.vars$Run), function(r){ x <- tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
# Transform them into MCMC objects (if the case):
if( sum(const.cols == 0) > 0 )
{
  # For glemann, keep only the non-constant variables:
  run_mcmc_g <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -union(1:3, 3+which(const.cols > 0)) ], thin=100)));
  
  # Compute and print the results:
  gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
  
  print(knitr::kable(gdiag$psrf, digits=c(4, 4), 
                     caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables."));
  cat("\n");
  
  cat(paste0("The multivariate *psrf* is ", round(gdiag$mpsrf,4), ".\n\n"));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ncol(run_mcmc[[1]])/3))}
if( !is.null(run_mcmc) ) plot(run_mcmc);
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across runs") +
  facet_grid(Variable ~ ., scales="free_y") + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
for( i in seq_along(run_mcmc) )
{
  print(knitr::kable(summary(run_mcmc[[i]])$statistics, digits=2, caption=paste0("Summaries for chain *",i,"*.")));
}
#knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  knitr::kable(as.data.frame(d[,c("Method", "Marginal_Likelihood")]), digits=c(NA, 2), row.names=FALSE, caption="Marginal likelihoods.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4*5, fig.cap="Reconstructed ancestral states throughout the tree (maximum posterior probability states or 'MAP')."}
# Ancestral trees:
plot_anc_states_trees(paste0("../output/anc_states/",tmp$anc.sts.trees));
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Percent of values reconstructed as '1' at the root."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- as.numeric(d$Perc.1.val) * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_point() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(d[,c("Character", "Perc.1.val")]), digits=c(NA, 2), row.names=FALSE, caption="Probability (as percent, %) to reconstruct a '1' at the root for each character.");
```


#### **R11**

##### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
r11.mcc.B <- tmp <- load.tree.results.for.model("R11mcc", trees.sample="B");
```

Convergence diagnostics and plots:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Convergence diagnostics:
run_mcmc <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ], thin=100)));
# Check and remove constant columns (they mess up gelman.diag):
const.cols <- colSums(do.call(rbind, lapply(unique(tmp$posterior.vars$Run), function(r){ x <- tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
# Transform them into MCMC objects (if the case):
if( sum(const.cols == 0) > 0 )
{
  # For glemann, keep only the non-constant variables:
  run_mcmc_g <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -union(1:3, 3+which(const.cols > 0)) ], thin=100)));
  
  # Compute and print the results:
  gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
  
  print(knitr::kable(gdiag$psrf, digits=c(4, 4), 
                     caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables."));
  cat("\n");
  
  cat(paste0("The multivariate *psrf* is ", round(gdiag$mpsrf,4), ".\n\n"));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ncol(run_mcmc[[1]])/3))}
if( !is.null(run_mcmc) ) plot(run_mcmc);
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across runs") +
  facet_grid(Variable ~ ., scales="free_y") + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
for( i in seq_along(run_mcmc) )
{
  print(knitr::kable(summary(run_mcmc[[i]])$statistics, digits=2, caption=paste0("Summaries for chain *",i,"*.")));
}
#knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  knitr::kable(as.data.frame(d[,c("Method", "Marginal_Likelihood")]), digits=c(NA, 2), row.names=FALSE, caption="Marginal likelihoods.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4*5, fig.cap="Reconstructed ancestral states throughout the tree (maximum posterior probability states or 'MAP')."}
# Ancestral trees:
plot_anc_states_trees(paste0("../output/anc_states/",tmp$anc.sts.trees));
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Percent of values reconstructed as '1' at the root."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- as.numeric(d$Perc.1.val) * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_point() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(d[,c("Character", "Perc.1.val")]), digits=c(NA, 2), row.names=FALSE, caption="Probability (as percent, %) to reconstruct a '1' at the root for each character.");
```

##### "C" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
r11.mcc.C <- tmp <- load.tree.results.for.model("R11mcc", trees.sample="C");
```

Convergence diagnostics and plots:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Convergence diagnostics:
run_mcmc <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ], thin=100)));
# Check and remove constant columns (they mess up gelman.diag):
const.cols <- colSums(do.call(rbind, lapply(unique(tmp$posterior.vars$Run), function(r){ x <- tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
# Transform them into MCMC objects (if the case):
if( sum(const.cols == 0) > 0 )
{
  # For glemann, keep only the non-constant variables:
  run_mcmc_g <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -union(1:3, 3+which(const.cols > 0)) ], thin=100)));
  
  # Compute and print the results:
  gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
  
  print(knitr::kable(gdiag$psrf, digits=c(4, 4), 
                     caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables."));
  cat("\n");
  
  cat(paste0("The multivariate *psrf* is ", round(gdiag$mpsrf,4), ".\n\n"));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ncol(run_mcmc[[1]])/3))}
if( !is.null(run_mcmc) ) plot(run_mcmc);
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across runs") +
  facet_grid(Variable ~ ., scales="free_y") + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
for( i in seq_along(run_mcmc) )
{
  print(knitr::kable(summary(run_mcmc[[i]])$statistics, digits=2, caption=paste0("Summaries for chain *",i,"*.")));
}
#knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  knitr::kable(as.data.frame(d[,c("Method", "Marginal_Likelihood")]), digits=c(NA, 2), row.names=FALSE, caption="Marginal likelihoods.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4*5, fig.cap="Reconstructed ancestral states throughout the tree (maximum posterior probability states or 'MAP')."}
# Ancestral trees:
plot_anc_states_trees(paste0("../output/anc_states/",tmp$anc.sts.trees));
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Percent of values reconstructed as '1' at the root."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- as.numeric(d$Perc.1.val) * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_point() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(d[,c("Character", "Perc.1.val")]), digits=c(NA, 2), row.names=FALSE, caption="Probability (as percent, %) to reconstruct a '1' at the root for each character.");
```


### **TU**

#### **T00**

##### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
t00.mcc.B <- tmp <- load.tree.results.for.model("T00mcc", trees.sample="B");
```

Convergence diagnostics and plots:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Convergence diagnostics:
run_mcmc <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ], thin=100)));
# Check and remove constant columns (they mess up gelman.diag):
const.cols <- colSums(do.call(rbind, lapply(unique(tmp$posterior.vars$Run), function(r){ x <- tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
# Transform them into MCMC objects (if the case):
if( sum(const.cols == 0) > 0 )
{
  # For glemann, keep only the non-constant variables:
  run_mcmc_g <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -union(1:3, 3+which(const.cols > 0)) ], thin=100)));
  
  # Compute and print the results:
  gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
  
  print(knitr::kable(gdiag$psrf, digits=c(4, 4), 
                     caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables."));
  cat("\n");
  
  cat(paste0("The multivariate *psrf* is ", round(gdiag$mpsrf,4), ".\n\n"));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ncol(run_mcmc[[1]])/3))}
if( !is.null(run_mcmc) ) plot(run_mcmc);
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across runs") +
  facet_grid(Variable ~ ., scales="free_y") + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
for( i in seq_along(run_mcmc) )
{
  print(knitr::kable(summary(run_mcmc[[i]])$statistics, digits=2, caption=paste0("Summaries for chain *",i,"*.")));
}
#knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  knitr::kable(as.data.frame(d[,c("Method", "Marginal_Likelihood")]), digits=c(NA, 2), row.names=FALSE, caption="Marginal likelihoods.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4*5, fig.cap="Reconstructed ancestral states throughout the tree (maximum posterior probability states or 'MAP')."}
# Ancestral trees:
plot_anc_states_trees(paste0("../output/anc_states/",tmp$anc.sts.trees));
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Percent of values reconstructed as '1' at the root."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- as.numeric(d$Perc.1.val) * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_point() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(d[,c("Character", "Perc.1.val")]), digits=c(NA, 2), row.names=FALSE, caption="Probability (as percent, %) to reconstruct a '1' at the root for each character.");
```

##### "C" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
t00.mcc.C <- tmp <- load.tree.results.for.model("T00mcc", trees.sample="C");
```

Convergence diagnostics and plots:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Convergence diagnostics:
run_mcmc <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ], thin=100)));
# Check and remove constant columns (they mess up gelman.diag):
const.cols <- colSums(do.call(rbind, lapply(unique(tmp$posterior.vars$Run), function(r){ x <- tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
# Transform them into MCMC objects (if the case):
if( sum(const.cols == 0) > 0 )
{
  # For glemann, keep only the non-constant variables:
  run_mcmc_g <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -union(1:3, 3+which(const.cols > 0)) ], thin=100)));
  
  # Compute and print the results:
  gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
  
  print(knitr::kable(gdiag$psrf, digits=c(4, 4), 
                     caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables."));
  cat("\n");
  
  cat(paste0("The multivariate *psrf* is ", round(gdiag$mpsrf,4), ".\n\n"));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ncol(run_mcmc[[1]])/3))}
if( !is.null(run_mcmc) ) plot(run_mcmc);
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across runs") +
  facet_grid(Variable ~ ., scales="free_y") + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
for( i in seq_along(run_mcmc) )
{
  print(knitr::kable(summary(run_mcmc[[i]])$statistics, digits=2, caption=paste0("Summaries for chain *",i,"*.")));
}
#knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  knitr::kable(as.data.frame(d[,c("Method", "Marginal_Likelihood")]), digits=c(NA, 2), row.names=FALSE, caption="Marginal likelihoods.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4*5, fig.cap="Reconstructed ancestral states throughout the tree (maximum posterior probability states or 'MAP')."}
# Ancestral trees:
plot_anc_states_trees(paste0("../output/anc_states/",tmp$anc.sts.trees));
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Percent of values reconstructed as '1' at the root."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- as.numeric(d$Perc.1.val) * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_point() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(d[,c("Character", "Perc.1.val")]), digits=c(NA, 2), row.names=FALSE, caption="Probability (as percent, %) to reconstruct a '1' at the root for each character.");
```


### **TUBR**

#### **B00**

##### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
b00.mcc.B <- tmp <- load.tree.results.for.model("B00mcc", trees.sample="B");
```

Convergence diagnostics and plots:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Convergence diagnostics:
run_mcmc <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ], thin=100)));
# Check and remove constant columns (they mess up gelman.diag):
const.cols <- colSums(do.call(rbind, lapply(unique(tmp$posterior.vars$Run), function(r){ x <- tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
# Transform them into MCMC objects (if the case):
if( sum(const.cols == 0) > 0 )
{
  # For glemann, keep only the non-constant variables:
  run_mcmc_g <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -union(1:3, 3+which(const.cols > 0)) ], thin=100)));
  
  # Compute and print the results:
  gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
  
  print(knitr::kable(gdiag$psrf, digits=c(4, 4), 
                     caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables."));
  cat("\n");
  
  cat(paste0("The multivariate *psrf* is ", round(gdiag$mpsrf,4), ".\n\n"));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ncol(run_mcmc[[1]])/3))}
if( !is.null(run_mcmc) ) plot(run_mcmc);
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across runs") +
  facet_grid(Variable ~ ., scales="free_y") + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
for( i in seq_along(run_mcmc) )
{
  print(knitr::kable(summary(run_mcmc[[i]])$statistics, digits=2, caption=paste0("Summaries for chain *",i,"*.")));
}
#knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  knitr::kable(as.data.frame(d[,c("Method", "Marginal_Likelihood")]), digits=c(NA, 2), row.names=FALSE, caption="Marginal likelihoods.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4*5, fig.cap="Reconstructed ancestral states throughout the tree (maximum posterior probability states or 'MAP')."}
# Ancestral trees:
plot_anc_states_trees(paste0("../output/anc_states/",tmp$anc.sts.trees));
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Percent of values reconstructed as '1' at the root."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- as.numeric(d$Perc.1.val) * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_point() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(d[,c("Character", "Perc.1.val")]), digits=c(NA, 2), row.names=FALSE, caption="Probability (as percent, %) to reconstruct a '1' at the root for each character.");
```

##### "C" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
b00.mcc.C <- tmp <- load.tree.results.for.model("B00mcc", trees.sample="C");
```

Convergence diagnostics and plots:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Convergence diagnostics:
run_mcmc <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ], thin=100)));
# Check and remove constant columns (they mess up gelman.diag):
const.cols <- colSums(do.call(rbind, lapply(unique(tmp$posterior.vars$Run), function(r){ x <- tmp$posterior.vars[ tmp$posterior.vars$Run == r, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
# Transform them into MCMC objects (if the case):
if( sum(const.cols == 0) > 0 )
{
  # For glemann, keep only the non-constant variables:
  run_mcmc_g <- as.mcmc.list(lapply(unique(tmp$posterior.vars$Run), function(r) as.mcmc(tmp$posterior.vars[ tmp$posterior.vars$Run == r, -union(1:3, 3+which(const.cols > 0)) ], thin=100)));
  
  # Compute and print the results:
  gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
  
  print(knitr::kable(gdiag$psrf, digits=c(4, 4), 
                     caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables."));
  cat("\n");
  
  cat(paste0("The multivariate *psrf* is ", round(gdiag$mpsrf,4), ".\n\n"));
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=2*5, fig.height=6*(ceiling(ncol(run_mcmc[[1]])/3))}
if( !is.null(run_mcmc) ) plot(run_mcmc);
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across runs") +
  facet_grid(Variable ~ ., scales="free_y") + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
for( i in seq_along(run_mcmc) )
{
  print(knitr::kable(summary(run_mcmc[[i]])$statistics, digits=2, caption=paste0("Summaries for chain *",i,"*.")));
}
#knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  knitr::kable(as.data.frame(d[,c("Method", "Marginal_Likelihood")]), digits=c(NA, 2), row.names=FALSE, caption="Marginal likelihoods.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4*5, fig.cap="Reconstructed ancestral states throughout the tree (maximum posterior probability states or 'MAP')."}
# Ancestral trees:
plot_anc_states_trees(paste0("../output/anc_states/",tmp$anc.sts.trees));
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Percent of values reconstructed as '1' at the root."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- as.numeric(d$Perc.1.val) * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_point() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(d[,c("Character", "Perc.1.val")]), digits=c(NA, 2), row.names=FALSE, caption="Probability (as percent, %) to reconstruct a '1' at the root for each character.");
```


### Model comparisons

#### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Assemble the likelihoods:
likelihoods.trees <- list("marginal"=rbind(m00.mcc.B$marg.likelihoods, m11.mcc.B$marg.likelihoods, 
                                           r00.mcc.B$marg.likelihoods, r11.mcc.B$marg.likelihoods, 
                                           t00.mcc.B$marg.likelihoods, b00.mcc.B$marg.likelihoods),
                          "posterior"=rbind(m00.mcc.B$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")], 
                                            m11.mcc.B$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")],
                                            r00.mcc.B$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")], 
                                            r11.mcc.B$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")],
                                            t00.mcc.B$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")], 
                                            b00.mcc.B$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")]));
```

Pseudo model comparison using likelihood, posterior and prior (works for all models):

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=7, fig.cap="(Pseudo) model comparison using ln(likelihood)."}
d <- melt(likelihoods.trees$posterior, id.vars="Model");
ggplot(d, aes(x=Model, y=value, color=Model)) + ylab("Value") +
  geom_boxplot() + 
  facet_grid(. ~ variable) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL;

# Pairwise comparions:
m1 <- aov(Likelihood ~ Model, data=likelihoods.trees$posterior);
knitr::kable(TukeyHSD(m1)$Model, digits=c(2, 2, 2, 4), caption="Pairwise differences in ln(likelihood) between models (showing Tukey's HSD multiple testing correction).");
```

Proper model comparison using Bayes Factors (does not work for all models):

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=7, fig.cap="Model comparison using ln(marginal likelihood)."}
d <- likelihoods.trees$marginal;
ggplot(d, aes(x=Model, y=Marginal_Likelihood, color=Model)) + ylab("ln(marginal likelihood)") +
  geom_boxplot() + 
  facet_grid(. ~ Method) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL;
```

Pairwise BFs for the corresponding individual trees, both as actual numbers (in the format: 'tree_id (BF)') and as plots (the horizontal lines represent the various degrees of strength of evidence for ln(BF): black = 0, between red and black = 'Barely worth mentioning', between red and blue = 'Substantial', between blue and dark green = 'Strong' and beyond dark green = 'Decisive'; positive favours the first model, negative the second model):

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=4, fig.height=4}
# Pairwise comparions:
BFs <- list();
m <- unique(likelihoods.trees$marginal$Model);
for(i in 1:(length(m)-1) )
{
  for(j in (i+1):length(m) )
  {
    cat(paste0("  - **",as.character(m[i]),"** - **",as.character(m[j]),"**:\n\n"));
    si <- likelihoods.trees$marginal$Model == m[i];
    sj <- likelihoods.trees$marginal$Model == m[j];
    shared.trees <- intersect(likelihoods.trees$marginal$Tree[ si ], 
                              likelihoods.trees$marginal$Tree[ sj ]);
    tmp <- lapply(unique(likelihoods.trees$marginal$Method), 
                  function(s)
                    {
                      cat(paste0("    + ",s,": "));
                      tmp <- vapply(shared.trees, 
                                     function(tree)
                                       { 
                                        st <- (likelihoods.trees$marginal$Tree == tree & likelihoods.trees$marginal$Method == s);
                                        return (likelihoods.trees$marginal$Marginal_Likelihood[si & st] - likelihoods.trees$marginal$Marginal_Likelihood[sj & st]);
                                       }, numeric(1));
                      names(tmp) <- shared.trees;
                      cat(paste0(names(tmp)," (",round(tmp,2),")", collapse=", ")); cat("\n");
                      return (tmp);
                    });
    names(tmp) <- unique(likelihoods.trees$marginal$Method);
    BFs[[length(BFs)+1]] <- tmp; names(BFs)[length(BFs)] <- paste0(m[i],"-",m[j]);
    cat("\n");
    
    d <- as.data.frame(do.call(rbind,tmp)); d$Method <- rownames(d); d <- melt(d, id.vars="Method");
    p <- ggplot(d, aes(x=Method, y=value, color=Method)) + 
      geom_hline(yintercept=0, color="black", linetype="solid") +
      geom_hline(yintercept=c(-1.16, 1.16), color="red", linetype="dotted") +
      geom_hline(yintercept=c(-2.30, 2.30), color="blue", linetype="dotted") +
      geom_hline(yintercept=c(-4.60, 4.60), color="darkgreen", linetype="dotted") +
      geom_boxplot() + geom_point(alpha=0.5) + ylab(paste0("ln(Bayes Factor) for ",as.character(m[i])," vs. ",as.character(m[j]))) + 
      NULL;
    print(p);
    cat("\n\n");
  }
}
```

#### "C" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Assemble the likelihoods:
likelihoods.trees <- list("marginal"=rbind(m00.mcc.C$marg.likelihoods, m11.mcc.C$marg.likelihoods, 
                                           r00.mcc.C$marg.likelihoods, r11.mcc.C$marg.likelihoods, 
                                           t00.mcc.C$marg.likelihoods, b00.mcc.C$marg.likelihoods),
                          "posterior"=rbind(m00.mcc.C$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")], 
                                            m11.mcc.C$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")],
                                            r00.mcc.C$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")], 
                                            r11.mcc.C$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")],
                                            t00.mcc.C$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")], 
                                            b00.mcc.C$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")]));
```

Pseudo model comparison using likelihood, posterior and prior (works for all models):

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=7, fig.cap="(Pseudo) model comparison using ln(likelihood)."}
d <- melt(likelihoods.trees$posterior, id.vars="Model");
ggplot(d, aes(x=Model, y=value, color=Model)) + ylab("Value") +
  geom_boxplot() + 
  facet_grid(. ~ variable) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL;

# Pairwise comparions:
m1 <- aov(Likelihood ~ Model, data=likelihoods.trees$posterior);
knitr::kable(TukeyHSD(m1)$Model, digits=c(2, 2, 2, 4), caption="Pairwise differences in ln(likelihood) between models (showing Tukey's HSD multiple testing correction).");
```

Proper model comparison using Bayes Factors (does not work for all models):

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=7, fig.cap="Model comparison using ln(marginal likelihood)."}
d <- likelihoods.trees$marginal;
ggplot(d, aes(x=Model, y=Marginal_Likelihood, color=Model)) + ylab("ln(marginal likelihood)") +
  geom_boxplot() + 
  facet_grid(. ~ Method) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL;
```

Pairwise BFs for the corresponding individual trees, both as actual numbers (in the format: 'tree_id (BF)') and as plots (the horizontal lines represent the various degrees of strength of evidence for ln(BF): black = 0, between red and black = 'Barely worth mentioning', between red and blue = 'Substantial', between blue and dark green = 'Strong' and beyond dark green = 'Decisive'; positive favours the first model, negative the second model):

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=4, fig.height=4}
# Pairwise comparions:
BFs <- list();
m <- unique(likelihoods.trees$marginal$Model);
for(i in 1:(length(m)-1) )
{
  for(j in (i+1):length(m) )
  {
    cat(paste0("  - **",as.character(m[i]),"** - **",as.character(m[j]),"**:\n\n"));
    si <- likelihoods.trees$marginal$Model == m[i];
    sj <- likelihoods.trees$marginal$Model == m[j];
    shared.trees <- intersect(likelihoods.trees$marginal$Tree[ si ], 
                              likelihoods.trees$marginal$Tree[ sj ]);
    tmp <- lapply(unique(likelihoods.trees$marginal$Method), 
                  function(s)
                    {
                      cat(paste0("    + ",s,": "));
                      tmp <- vapply(shared.trees, 
                                     function(tree)
                                       { 
                                        st <- (likelihoods.trees$marginal$Tree == tree & likelihoods.trees$marginal$Method == s);
                                        return (likelihoods.trees$marginal$Marginal_Likelihood[si & st] - likelihoods.trees$marginal$Marginal_Likelihood[sj & st]);
                                       }, numeric(1));
                      names(tmp) <- shared.trees;
                      cat(paste0(names(tmp)," (",round(tmp,2),")", collapse=", ")); cat("\n");
                      return (tmp);
                    });
    names(tmp) <- unique(likelihoods.trees$marginal$Method);
    BFs[[length(BFs)+1]] <- tmp; names(BFs)[length(BFs)] <- paste0(m[i],"-",m[j]);
    cat("\n");
    
    d <- as.data.frame(do.call(rbind,tmp)); d$Method <- rownames(d); d <- melt(d, id.vars="Method");
    p <- ggplot(d, aes(x=Method, y=value, color=Method)) + 
      geom_hline(yintercept=0, color="black", linetype="solid") +
      geom_hline(yintercept=c(-1.16, 1.16), color="red", linetype="dotted") +
      geom_hline(yintercept=c(-2.30, 2.30), color="blue", linetype="dotted") +
      geom_hline(yintercept=c(-4.60, 4.60), color="darkgreen", linetype="dotted") +
      geom_boxplot() + geom_point(alpha=0.5) + ylab(paste0("ln(Bayes Factor) for ",as.character(m[i])," vs. ",as.character(m[j]))) + 
      NULL;
    print(p);
    cat("\n\n");
  }
}
```



### Summary

#### "B" trees

Individual models: 

  - **M00**: perfect convergence, very similar params, very good mixing, chains are settling, ancestral states ~0% "1", `r12` ~ 0.6
  - **M11**: very good convergence for `root_freq`, `cats_r12` and `beta_cats_rates` but very bad for the others, espeically for `site_rates`, most params are very similar (`site_rates[1]` and `site_rates[2]` are actually inverted), good mixing, chains seem to be settling, ancestral states ~0% "1", `alpha_site_rates` either very small (~0.09) or very large (~500000), `beta_cats_rates` ~ 0.96 or 1.80, `cats_r12` one very low and one very high
  - **R00**: perfect convergence, very similar params, very good mixing, chains are settling, ancestral states most ~15%, two ~30% and 1 (char3) at 73% "1", `r12` ~ 1.33, `n_cats` ~ 7.5
  - **R11**: very good convergence, very similar params, good mixing, chains are settling, ancestral states most ~0%, 1 (char3) at ~2% "1", `alpha_site_rates` very different, `beta_cats_rates` ~1.3, `cats_r12` very similar (~0.2 and ~0.8), `n_cats` ~6.1
  - **T00**: perfect convergence, very similar params, very good mixing, chains are settling, ancestral states ~50% "1", `r12` ~ 2.3, `n_cats` = 1, `concentration` ~ 0.21 
  - **B00**: perfect convergence, very similar params, very good mixing, chains are settling, ancestral states ~0% "1", `r12` ~ 0.4, `n_cats` ~ 2, `concentration` ~ 0.3, `base_rate` ~ 0.01 

Model (pseudo-)comparison: 

  - **T00** is clearly the worst by far, **M00**, **R00** and **M11** next, and **R11** the best, with **B00** apparently comparable to **R11**.

#### "C" trees

Individual models: 

  - **M00**: perfect convergence, very similar params, very good mixing, chains are settling, ancestral states ~0% "1", `r12` ~ 0.6
  - **M11**: very good convergence, params are quite similar, relatively good mixing, chains seem to be settling, ancestral states ~0% "1", `alpha_site_rates` pretty large (~300000), `beta_cats_rates` ~ 1.6 or 1.9, `cats_r12` one low (~0.2) and one high (~0.8)
  - **R00**: perfect convergence, very similar params, very good mixing, chains are settling, ancestral states most at ~15-20% except 1 (char3) at 50% "1", `r12` ~ 1.15, `n_cats` ~ 7.7
  - **R11**: perfect convergence, very similar params, very good mixing, chains are settling, chains are settling, ancestral states most ~0%, 1 (char3) at ~1% "1", `alpha_site_rates` ~0.18, `beta_cats_rates` ~1.45, `cats_r12` very similar (~0.2 and ~0.8), `n_cats` ~6.0
  - **T00**: perfect convergence, very similar params, very good mixing, chains are settling, ancestral states ~50% "1", `r12` ~ 2.3, `n_cats` = 1, `concentration` ~ 0.21 
  - **B00**: perfect convergence, very similar params, very good mixing, chains are settling, ancestral states ~0% "1", `r12` ~ 0.4, `n_cats` ~ 2, `concentration` ~ 0.3, `base_rate` ~ 0.01 

Model (pseudo-)comparison: 

  - **T00** is clearly the worst by far, **M00**, **R00** and **M11** next, and **R11** and **B00** the best.


### Interpretation

#### The original T&U model is deviant ...

The original T&U model simultaneously fits the data *the worst* and produces the *most deviant* estimates of the ancestral states, `n_cats` and `r12`.

#### ... but adding base rate back greatly improves it

Again, adding back the base rate to the original T&U model (as in the original `RevBayes` tutorial from which this was inspired) simultaneously:

  - *improves its fit* to the data (on average, from about ~ -290 for **T00** to ~ -200 for **B00** for 1 extra numeric parameter, corresponding to a $\Delta AIC \approx (2k - 2 \times (-290)) - (2(k+1) - 2 \times (-200))$ = $2 \times 290 -2 - 2 \times 200$ = $178$ AIC points), possibly making it the top (or at least not mucb worse than the best) model,
  - results in *ancestral state estimates at the root* of overwhelming absence of labiodentals (~0% reconstructed presence; in line with the original paper and the other models) from the argubaly wrong 50% for the original T&U model,
  - *increases `n_cats`* from 1 to 2, more in line with our original results and the other models.

Thus, unless theoretically strongly motivated, excluding the base rate (as done by the original T&U model) strongly distorts the results.

#### Other models (**MC** and **RMC**)

The simplest molecular clock **MC** and its relaxed variant **RMC** are pretty bad fits to the data (but not as bad as the original **TU**), but adding better root models and allowing between-site variation greatly imrpoves the fit of **RMC**.

#### Probability of labiodentals at the root and changes in rates

All models except the original T&U reconstruct an *absence of labiodentals at the root* (some -- including **B00** -- with almost 100% probability) and a number of *branch rate categories* clearly greater than 1.





## Each tree independently

Here we look at the results of the models that take an individual tree as given; please note that due to the high computational costs, I managed to apply this to only a small number of trees (variable across models), so these results should be taken as suggestive.

Due to computational costs, I only managed to use the "B" sample...
Of course, given that all the code is available and easy to use, one can in principle run both samples in full, but this must be left for the future...

Here, I am focusing only on:

  - estimates of convergence (for those trees that have two runs)
  - estimates across trees for the prior, likelikood and posterior, marginal likelihood (for those models that have it), and the relevant parameters
  - model (pseudo-)comparisons

Thus, I am not plotting and summarizing the covergence and distributions of parameters for each tree as this would unnecessairly clutter the report, but these can of course be seen using, for example, `Tracer`.

For these runs, we used the same `RevBayes` params as for the MCC consensus trees.

### **MC**

#### **M00**

##### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
m00.trees.B <- tmp <- load.tree.results.for.model("M00", trees.sample="B");
```

We ran `r length(unique(tmp$posterior.vars$Tree))` individual trees, of which `r length(unique(tmp$posterior.vars$Tree[tmp$posterior.vars$Run != 1]))` had more than 1 run; `r ifelse(is.null(tmp$marg.likelihoods), 0, length(unique(tmp$marg.likelihoods$Tree)))` trees have marginal likelihood estimates. 

Convergence diagnostics:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Trees and runs:
trees.and.runs <- unique(tmp$posterior.vars[,c("Tree","Run")]); 
trees.and.runs$Tree <- as.numeric(as.character(trees.and.runs$Tree)); trees.and.runs$Run <- as.numeric(as.character(trees.and.runs$Run)); 
trees.and.runs <- trees.and.runs[ order(trees.and.runs$Tree, trees.and.runs$Run), ];

# Convergence diagnostics:
trees.with.two.runs <- unique(trees.and.runs$Tree[ trees.and.runs$Run != 1 ]);
psrfs <- do.call(rbind, lapply(trees.with.two.runs, function(tree)
  {
    # Check and remove constant columns (they mess up gelman.diag):
    const.cols <- colSums(do.call(rbind, lapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) { x <- tmp$posterior.vars[tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
    # Transform them into MCMC objects (if the case):
    if( sum(const.cols == 0) > 0 )
    {
      # For gelman, keep only the non-constant variables and make sure all chains have the same length:
      min.chain.len <- min(vapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) sum(tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, na.rm=TRUE), numeric(1)), na.rm=TRUE);
      run_mcmc_g <- as.mcmc.list(lapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) as.mcmc(tmp$posterior.vars[tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, -union(1:3, 3+which(const.cols > 0)) ][1:min.chain.len,], thin=100)));
      
      # Compute and print the results:
      gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
      
      # Return the values as a data.frame:
      return (data.frame("tree"=tree, "var"=c(rownames(gdiag$psrf), "Multivariate"), "psrf"=c(gdiag$psrf[,1], gdiag$mpsrf), "psrf.upp.ci"=c(gdiag$psrf[,2], NA)));
    }
  }));
knitr::kable(psrfs, row.names=FALSE, col.names=c("Tree", "Variable", "Point est.", "Upper C.I."), digits=c(0, NA, 4, 4),
             caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables for each tree with more than one run.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=1*(ncol(tmp$posterior.vars)-3), fig.height=5, fig.cap="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables across trees with more than one run."}
ggplot(psrfs, aes(x=var, y=psrf, color=var)) + 
  geom_boxplot() + geom_point(alpha=0.5) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL; 
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across trees and runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across trees and runs") +
  facet_grid(Variable ~ ., scales="free_y") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across all trees and runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Marginal likelihood across trees and runs."}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  ggplot(d, aes(x=Method, y=Marginal_Likelihood, color=Method)) + 
    geom_violin() + geom_boxplot(alpha=0.25) + geom_point(alpha=0.5) + 
    ylab("ln(marginal likelihood)") +
    NULL;
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods %>% 
    group_by(Model, Method) %>% 
    summarise("min"=min(Marginal_Likelihood, na.rm=TRUE), "max"=max(Marginal_Likelihood, na.rm=TRUE),
              "mean"=mean(Marginal_Likelihood, na.rm=TRUE), "median"=median(Marginal_Likelihood, na.rm=TRUE),
              "sd"=sd(Marginal_Likelihood, na.rm=TRUE), "iqr"=IQR(Marginal_Likelihood, na.rm=TRUE));
  knitr::kable(as.data.frame(d), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of marginal likelihoods across all trees and runs.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=7, fig.cap="Percent of values reconstructed as '1' at the root across trees and runs."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- d$Perc.1.val * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_boxplot() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
d <- d %>% 
  group_by(Model, Character) %>% 
  summarise("min"=min(Perc.1.val, na.rm=TRUE), "max"=max(Perc.1.val, na.rm=TRUE),
            "mean"=mean(Perc.1.val, na.rm=TRUE), "median"=median(Perc.1.val, na.rm=TRUE),
            "sd"=sd(Perc.1.val, na.rm=TRUE), "iqr"=IQR(Perc.1.val, na.rm=TRUE));
knitr::kable(as.data.frame(d), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of the probability (as percent, %) to reconstruct a '1' at the root for each character across all trees and runs.");
```


#### **M11**

##### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
m11.trees.B <- tmp <- load.tree.results.for.model("M11", trees.sample="B");
```

We ran `r length(unique(tmp$posterior.vars$Tree))` individual trees, of which `r length(unique(tmp$posterior.vars$Tree[tmp$posterior.vars$Run != 1]))` had more than 1 run; `r ifelse(is.null(tmp$marg.likelihoods), 0, length(unique(tmp$marg.likelihoods$Tree)))` trees have marginal likelihood estimates. 

Convergence diagnostics:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Trees and runs:
trees.and.runs <- unique(tmp$posterior.vars[,c("Tree","Run")]); 
trees.and.runs$Tree <- as.numeric(as.character(trees.and.runs$Tree)); trees.and.runs$Run <- as.numeric(as.character(trees.and.runs$Run)); 
trees.and.runs <- trees.and.runs[ order(trees.and.runs$Tree, trees.and.runs$Run), ];

# Convergence diagnostics:
trees.with.two.runs <- unique(trees.and.runs$Tree[ trees.and.runs$Run != 1 ]);
psrfs <- do.call(rbind, lapply(trees.with.two.runs, function(tree)
  {
    # Check and remove constant columns (they mess up gelman.diag):
    const.cols <- colSums(do.call(rbind, lapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) { x <- tmp$posterior.vars[tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
    # Transform them into MCMC objects (if the case):
    if( sum(const.cols == 0) > 0 )
    {
      # For gelman, keep only the non-constant variables and make sure all chains have the same length:
      min.chain.len <- min(vapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) sum(tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, na.rm=TRUE), numeric(1)), na.rm=TRUE);
      run_mcmc_g <- as.mcmc.list(lapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) as.mcmc(tmp$posterior.vars[tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, -union(1:3, 3+which(const.cols > 0)) ][1:min.chain.len,], thin=100)));
      
      # Compute and print the results:
      gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
      
      # Return the values as a data.frame:
      return (data.frame("tree"=tree, "var"=c(rownames(gdiag$psrf), "Multivariate"), "psrf"=c(gdiag$psrf[,1], gdiag$mpsrf), "psrf.upp.ci"=c(gdiag$psrf[,2], NA)));
    }
  }));
knitr::kable(psrfs, row.names=FALSE, col.names=c("Tree", "Variable", "Point est.", "Upper C.I."), digits=c(0, NA, 4, 4),
             caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables for each tree with more than one run.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=1*(ncol(tmp$posterior.vars)-3), fig.height=5, fig.cap="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables across trees with more than one run."}
ggplot(psrfs, aes(x=var, y=psrf, color=var)) + 
  geom_boxplot() + geom_point(alpha=0.5) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL; 
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across trees and runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across trees and runs") +
  facet_grid(Variable ~ ., scales="free_y") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across all trees and runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Marginal likelihood across trees and runs."}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  ggplot(d, aes(x=Method, y=Marginal_Likelihood, color=Method)) + 
    geom_violin() + geom_boxplot(alpha=0.25) + geom_point(alpha=0.5) + 
    ylab("ln(marginal likelihood)") +
    NULL;
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods %>% 
    group_by(Model, Method) %>% 
    summarise("min"=min(Marginal_Likelihood, na.rm=TRUE), "max"=max(Marginal_Likelihood, na.rm=TRUE),
              "mean"=mean(Marginal_Likelihood, na.rm=TRUE), "median"=median(Marginal_Likelihood, na.rm=TRUE),
              "sd"=sd(Marginal_Likelihood, na.rm=TRUE), "iqr"=IQR(Marginal_Likelihood, na.rm=TRUE));
  knitr::kable(as.data.frame(d), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of marginal likelihoods across all trees and runs.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=7, fig.cap="Percent of values reconstructed as '1' at the root across trees and runs."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- d$Perc.1.val * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_boxplot() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
d <- d %>% 
  group_by(Model, Character) %>% 
  summarise("min"=min(Perc.1.val, na.rm=TRUE), "max"=max(Perc.1.val, na.rm=TRUE),
            "mean"=mean(Perc.1.val, na.rm=TRUE), "median"=median(Perc.1.val, na.rm=TRUE),
            "sd"=sd(Perc.1.val, na.rm=TRUE), "iqr"=IQR(Perc.1.val, na.rm=TRUE));
knitr::kable(as.data.frame(d), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of the probability (as percent, %) to reconstruct a '1' at the root for each character across all trees and runs.");
```


### **RMC**

#### **R00**

##### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
r00.trees.B <- tmp <- load.tree.results.for.model("R00", trees.sample="B");
```

We ran `r length(unique(tmp$posterior.vars$Tree))` individual trees, of which `r length(unique(tmp$posterior.vars$Tree[tmp$posterior.vars$Run != 1]))` had more than 1 run; `r ifelse(is.null(tmp$marg.likelihoods), 0, length(unique(tmp$marg.likelihoods$Tree)))` trees have marginal likelihood estimates. 

Convergence diagnostics:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Trees and runs:
trees.and.runs <- unique(tmp$posterior.vars[,c("Tree","Run")]); 
trees.and.runs$Tree <- as.numeric(as.character(trees.and.runs$Tree)); trees.and.runs$Run <- as.numeric(as.character(trees.and.runs$Run)); 
trees.and.runs <- trees.and.runs[ order(trees.and.runs$Tree, trees.and.runs$Run), ];

# Convergence diagnostics:
trees.with.two.runs <- unique(trees.and.runs$Tree[ trees.and.runs$Run != 1 ]);
psrfs <- do.call(rbind, lapply(trees.with.two.runs, function(tree)
  {
    # Check and remove constant columns (they mess up gelman.diag):
    const.cols <- colSums(do.call(rbind, lapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) { x <- tmp$posterior.vars[tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
    # Transform them into MCMC objects (if the case):
    if( sum(const.cols == 0) > 0 )
    {
      # For gelman, keep only the non-constant variables and make sure all chains have the same length:
      min.chain.len <- min(vapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) sum(tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, na.rm=TRUE), numeric(1)), na.rm=TRUE);
      run_mcmc_g <- as.mcmc.list(lapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) as.mcmc(tmp$posterior.vars[tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, -union(1:3, 3+which(const.cols > 0)) ][1:min.chain.len,], thin=100)));
      
      # Compute and print the results:
      gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
      
      # Return the values as a data.frame:
      return (data.frame("tree"=tree, "var"=c(rownames(gdiag$psrf), "Multivariate"), "psrf"=c(gdiag$psrf[,1], gdiag$mpsrf), "psrf.upp.ci"=c(gdiag$psrf[,2], NA)));
    }
  }));
knitr::kable(psrfs, row.names=FALSE, col.names=c("Tree", "Variable", "Point est.", "Upper C.I."), digits=c(0, NA, 4, 4),
             caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables for each tree with more than one run.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=1*(ncol(tmp$posterior.vars)-3), fig.height=5, fig.cap="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables across trees with more than one run."}
ggplot(psrfs, aes(x=var, y=psrf, color=var)) + 
  geom_boxplot() + geom_point(alpha=0.5) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL; 
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across trees and runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across trees and runs") +
  facet_grid(Variable ~ ., scales="free_y") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across all trees and runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Marginal likelihood across trees and runs."}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  ggplot(d, aes(x=Method, y=Marginal_Likelihood, color=Method)) + 
    geom_violin() + geom_boxplot(alpha=0.25) + geom_point(alpha=0.5) + 
    ylab("ln(marginal likelihood)") +
    NULL;
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods %>% 
    group_by(Model, Method) %>% 
    summarise("min"=min(Marginal_Likelihood, na.rm=TRUE), "max"=max(Marginal_Likelihood, na.rm=TRUE),
              "mean"=mean(Marginal_Likelihood, na.rm=TRUE), "median"=median(Marginal_Likelihood, na.rm=TRUE),
              "sd"=sd(Marginal_Likelihood, na.rm=TRUE), "iqr"=IQR(Marginal_Likelihood, na.rm=TRUE));
  knitr::kable(as.data.frame(d), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of marginal likelihoods across all trees and runs.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=7, fig.cap="Percent of values reconstructed as '1' at the root across trees and runs."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- d$Perc.1.val * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_boxplot() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
d <- d %>% 
  group_by(Model, Character) %>% 
  summarise("min"=min(Perc.1.val, na.rm=TRUE), "max"=max(Perc.1.val, na.rm=TRUE),
            "mean"=mean(Perc.1.val, na.rm=TRUE), "median"=median(Perc.1.val, na.rm=TRUE),
            "sd"=sd(Perc.1.val, na.rm=TRUE), "iqr"=IQR(Perc.1.val, na.rm=TRUE));
knitr::kable(as.data.frame(d), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of the probability (as percent, %) to reconstruct a '1' at the root for each character across all trees and runs.");
```


#### **R11**

##### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
r11.trees.B <- tmp <- load.tree.results.for.model("R11", trees.sample="B");
```

We ran `r length(unique(tmp$posterior.vars$Tree))` individual trees, of which `r length(unique(tmp$posterior.vars$Tree[tmp$posterior.vars$Run != 1]))` had more than 1 run; `r ifelse(is.null(tmp$marg.likelihoods), 0, length(unique(tmp$marg.likelihoods$Tree)))` trees have marginal likelihood estimates. 

Convergence diagnostics:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Trees and runs:
trees.and.runs <- unique(tmp$posterior.vars[,c("Tree","Run")]); 
trees.and.runs$Tree <- as.numeric(as.character(trees.and.runs$Tree)); trees.and.runs$Run <- as.numeric(as.character(trees.and.runs$Run)); 
trees.and.runs <- trees.and.runs[ order(trees.and.runs$Tree, trees.and.runs$Run), ];

# Convergence diagnostics:
trees.with.two.runs <- unique(trees.and.runs$Tree[ trees.and.runs$Run != 1 ]);
psrfs <- do.call(rbind, lapply(trees.with.two.runs, function(tree)
  {
    # Check and remove constant columns (they mess up gelman.diag):
    const.cols <- colSums(do.call(rbind, lapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) { x <- tmp$posterior.vars[tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
    # Transform them into MCMC objects (if the case):
    if( sum(const.cols == 0) > 0 )
    {
      # For gelman, keep only the non-constant variables and make sure all chains have the same length:
      min.chain.len <- min(vapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) sum(tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, na.rm=TRUE), numeric(1)), na.rm=TRUE);
      run_mcmc_g <- as.mcmc.list(lapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) as.mcmc(tmp$posterior.vars[tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, -union(1:3, 3+which(const.cols > 0)) ][1:min.chain.len,], thin=100)));
      
      # Compute and print the results:
      gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
      
      # Return the values as a data.frame:
      return (data.frame("tree"=tree, "var"=c(rownames(gdiag$psrf), "Multivariate"), "psrf"=c(gdiag$psrf[,1], gdiag$mpsrf), "psrf.upp.ci"=c(gdiag$psrf[,2], NA)));
    }
  }));
knitr::kable(psrfs, row.names=FALSE, col.names=c("Tree", "Variable", "Point est.", "Upper C.I."), digits=c(0, NA, 4, 4),
             caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables for each tree with more than one run.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=1*(ncol(tmp$posterior.vars)-3), fig.height=5, fig.cap="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables across trees with more than one run."}
ggplot(psrfs, aes(x=var, y=psrf, color=var)) + 
  geom_boxplot() + geom_point(alpha=0.5) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL; 
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across trees and runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across trees and runs") +
  facet_grid(Variable ~ ., scales="free_y") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across all trees and runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Marginal likelihood across trees and runs."}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  ggplot(d, aes(x=Method, y=Marginal_Likelihood, color=Method)) + 
    geom_violin() + geom_boxplot(alpha=0.25) + geom_point(alpha=0.5) + 
    ylab("ln(marginal likelihood)") +
    NULL;
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods %>% 
    group_by(Model, Method) %>% 
    summarise("min"=min(Marginal_Likelihood, na.rm=TRUE), "max"=max(Marginal_Likelihood, na.rm=TRUE),
              "mean"=mean(Marginal_Likelihood, na.rm=TRUE), "median"=median(Marginal_Likelihood, na.rm=TRUE),
              "sd"=sd(Marginal_Likelihood, na.rm=TRUE), "iqr"=IQR(Marginal_Likelihood, na.rm=TRUE));
  knitr::kable(as.data.frame(d), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of marginal likelihoods across all trees and runs.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=7, fig.cap="Percent of values reconstructed as '1' at the root across trees and runs."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- d$Perc.1.val * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_boxplot() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
d <- d %>% 
  group_by(Model, Character) %>% 
  summarise("min"=min(Perc.1.val, na.rm=TRUE), "max"=max(Perc.1.val, na.rm=TRUE),
            "mean"=mean(Perc.1.val, na.rm=TRUE), "median"=median(Perc.1.val, na.rm=TRUE),
            "sd"=sd(Perc.1.val, na.rm=TRUE), "iqr"=IQR(Perc.1.val, na.rm=TRUE));
knitr::kable(as.data.frame(d), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of the probability (as percent, %) to reconstruct a '1' at the root for each character across all trees and runs.");
```


### **TU**

#### **T00**

##### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
t00.trees.B <- tmp <- load.tree.results.for.model("T00", trees.sample="B");
```

We ran `r length(unique(tmp$posterior.vars$Tree))` individual trees, of which `r length(unique(tmp$posterior.vars$Tree[tmp$posterior.vars$Run != 1]))` had more than 1 run; `r ifelse(is.null(tmp$marg.likelihoods), 0, length(unique(tmp$marg.likelihoods$Tree)))` trees have marginal likelihood estimates. 

Convergence diagnostics:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Trees and runs:
trees.and.runs <- unique(tmp$posterior.vars[,c("Tree","Run")]); 
trees.and.runs$Tree <- as.numeric(as.character(trees.and.runs$Tree)); trees.and.runs$Run <- as.numeric(as.character(trees.and.runs$Run)); 
trees.and.runs <- trees.and.runs[ order(trees.and.runs$Tree, trees.and.runs$Run), ];

# Convergence diagnostics:
trees.with.two.runs <- unique(trees.and.runs$Tree[ trees.and.runs$Run != 1 ]);
psrfs <- do.call(rbind, lapply(trees.with.two.runs, function(tree)
  {
    # Check and remove constant columns (they mess up gelman.diag):
    const.cols <- colSums(do.call(rbind, lapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) { x <- tmp$posterior.vars[tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
    # Transform them into MCMC objects (if the case):
    if( sum(const.cols == 0) > 0 )
    {
      # For gelman, keep only the non-constant variables and make sure all chains have the same length:
      min.chain.len <- min(vapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) sum(tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, na.rm=TRUE), numeric(1)), na.rm=TRUE);
      run_mcmc_g <- as.mcmc.list(lapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) as.mcmc(tmp$posterior.vars[tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, -union(1:3, 3+which(const.cols > 0)) ][1:min.chain.len,], thin=100)));
      
      # Compute and print the results:
      gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
      
      # Return the values as a data.frame:
      return (data.frame("tree"=tree, "var"=c(rownames(gdiag$psrf), "Multivariate"), "psrf"=c(gdiag$psrf[,1], gdiag$mpsrf), "psrf.upp.ci"=c(gdiag$psrf[,2], NA)));
    }
  }));
knitr::kable(psrfs, row.names=FALSE, col.names=c("Tree", "Variable", "Point est.", "Upper C.I."), digits=c(0, NA, 4, 4),
             caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables for each tree with more than one run.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=1*(ncol(tmp$posterior.vars)-3), fig.height=5, fig.cap="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables across trees with more than one run."}
ggplot(psrfs, aes(x=var, y=psrf, color=var)) + 
  geom_boxplot() + geom_point(alpha=0.5) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL; 
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across trees and runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across trees and runs") +
  facet_grid(Variable ~ ., scales="free_y") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across all trees and runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Marginal likelihood across trees and runs."}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  ggplot(d, aes(x=Method, y=Marginal_Likelihood, color=Method)) + 
    geom_violin() + geom_boxplot(alpha=0.25) + geom_point(alpha=0.5) + 
    ylab("ln(marginal likelihood)") +
    NULL;
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods %>% 
    group_by(Model, Method) %>% 
    summarise("min"=min(Marginal_Likelihood, na.rm=TRUE), "max"=max(Marginal_Likelihood, na.rm=TRUE),
              "mean"=mean(Marginal_Likelihood, na.rm=TRUE), "median"=median(Marginal_Likelihood, na.rm=TRUE),
              "sd"=sd(Marginal_Likelihood, na.rm=TRUE), "iqr"=IQR(Marginal_Likelihood, na.rm=TRUE));
  knitr::kable(as.data.frame(d), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of marginal likelihoods across all trees and runs.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=7, fig.cap="Percent of values reconstructed as '1' at the root across trees and runs."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- d$Perc.1.val * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_boxplot() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
d <- d %>% 
  group_by(Model, Character) %>% 
  summarise("min"=min(Perc.1.val, na.rm=TRUE), "max"=max(Perc.1.val, na.rm=TRUE),
            "mean"=mean(Perc.1.val, na.rm=TRUE), "median"=median(Perc.1.val, na.rm=TRUE),
            "sd"=sd(Perc.1.val, na.rm=TRUE), "iqr"=IQR(Perc.1.val, na.rm=TRUE));
knitr::kable(as.data.frame(d), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of the probability (as percent, %) to reconstruct a '1' at the root for each character across all trees and runs.");
```


### **TUBR**

#### **B00**

##### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
b00.trees.B <- tmp <- load.tree.results.for.model("B00", trees.sample="B");
```

We ran `r length(unique(tmp$posterior.vars$Tree))` individual trees, of which `r length(unique(tmp$posterior.vars$Tree[tmp$posterior.vars$Run != 1]))` had more than 1 run; `r ifelse(is.null(tmp$marg.likelihoods), 0, length(unique(tmp$marg.likelihoods$Tree)))` trees have marginal likelihood estimates. 

Convergence diagnostics:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Trees and runs:
trees.and.runs <- unique(tmp$posterior.vars[,c("Tree","Run")]); 
trees.and.runs$Tree <- as.numeric(as.character(trees.and.runs$Tree)); trees.and.runs$Run <- as.numeric(as.character(trees.and.runs$Run)); 
trees.and.runs <- trees.and.runs[ order(trees.and.runs$Tree, trees.and.runs$Run), ];

# Convergence diagnostics:
trees.with.two.runs <- unique(trees.and.runs$Tree[ trees.and.runs$Run != 1 ]);
psrfs <- do.call(rbind, lapply(trees.with.two.runs, function(tree)
  {
    # Check and remove constant columns (they mess up gelman.diag):
    const.cols <- colSums(do.call(rbind, lapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) { x <- tmp$posterior.vars[tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, -(1:3) ]; vapply(1:ncol(x), function(i) as.numeric(var(x[,i], na.rm=TRUE)==0), numeric(1))} )));
    # Transform them into MCMC objects (if the case):
    if( sum(const.cols == 0) > 0 )
    {
      # For gelman, keep only the non-constant variables and make sure all chains have the same length:
      min.chain.len <- min(vapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) sum(tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, na.rm=TRUE), numeric(1)), na.rm=TRUE);
      run_mcmc_g <- as.mcmc.list(lapply(unique(trees.and.runs$Run[trees.and.runs$Tree == tree]), function(run) as.mcmc(tmp$posterior.vars[tmp$posterior.vars$Tree == tree & tmp$posterior.vars$Run == run, -union(1:3, 3+which(const.cols > 0)) ][1:min.chain.len,], thin=100)));
      
      # Compute and print the results:
      gdiag <- gelman.diag(run_mcmc_g, autoburnin=FALSE);
      
      # Return the values as a data.frame:
      return (data.frame("tree"=tree, "var"=c(rownames(gdiag$psrf), "Multivariate"), "psrf"=c(gdiag$psrf[,1], gdiag$mpsrf), "psrf.upp.ci"=c(gdiag$psrf[,2], NA)));
    }
  }));
knitr::kable(psrfs, row.names=FALSE, col.names=c("Tree", "Variable", "Point est.", "Upper C.I."), digits=c(0, NA, 4, 4),
             caption="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables for each tree with more than one run.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=1*(ncol(tmp$posterior.vars)-3), fig.height=5, fig.cap="**Gelman and Rubin's covergence diagnostics** (*potential scale reduction factors* or *psrf*s; convergence is indicated by values close to 1.0) for the relevant variables across trees with more than one run."}
ggplot(psrfs, aes(x=var, y=psrf, color=var)) + 
  geom_boxplot() + geom_point(alpha=0.5) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL; 
```

Summaries of posterior distributions of the relevant variables across trees and runs:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=2*(ncol(tmp$posterior.vars)-3), fig.cap="Posterior distribution of relevant variables across trees and runs."}
# Summary of relevant variables:
d <- cbind(tmp$posterior.vars[,-c(2,3)], "Tree.Run"=paste0(tmp$posterior.vars$Tree,".",tmp$posterior.vars$Run));
d <- melt(d, id.vars=c("Model", "Tree.Run"), variable.name="Variable", value.name="Value");
ggplot(d, aes(x=Tree.Run, y=Value, color=Variable, fill=Variable)) + 
  geom_violin() + xlab("Tree and Run") + ylab("Variable's values") + ggtitle("Posterior distributions across trees and runs") +
  facet_grid(Variable ~ ., scales="free_y") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(as.data.frame(tmp$summaries.vars$overall), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of posterior distribution of the relevant variables across all trees and runs.");
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, fig.cap="Marginal likelihood across trees and runs."}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods;
  ggplot(d, aes(x=Method, y=Marginal_Likelihood, color=Method)) + 
    geom_violin() + geom_boxplot(alpha=0.25) + geom_point(alpha=0.5) + 
    ylab("ln(marginal likelihood)") +
    NULL;
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Marginal likelihoods:
if( !is.null(tmp$marg.likelihoods) )
{
  d <- tmp$marg.likelihoods %>% 
    group_by(Model, Method) %>% 
    summarise("min"=min(Marginal_Likelihood, na.rm=TRUE), "max"=max(Marginal_Likelihood, na.rm=TRUE),
              "mean"=mean(Marginal_Likelihood, na.rm=TRUE), "median"=median(Marginal_Likelihood, na.rm=TRUE),
              "sd"=sd(Marginal_Likelihood, na.rm=TRUE), "iqr"=IQR(Marginal_Likelihood, na.rm=TRUE));
  knitr::kable(as.data.frame(d), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of marginal likelihoods across all trees and runs.");
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=7, fig.cap="Percent of values reconstructed as '1' at the root across trees and runs."}
# Ancestral values:
d <- melt(tmp$prop.1.at.root, id.vars=c("Model", "Tree", "Run"), variable.name="Character", value.name="Perc.1.val"); d$Perc.1.val <- d$Perc.1.val * 100.0;
ggplot(d, aes(x=Character, y=Perc.1.val, color=Character)) + ylim(c(0,100)) + ylab("% reconstructed '1' at root") +
  geom_boxplot() + 
  NULL;
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
d <- d %>% 
  group_by(Model, Character) %>% 
  summarise("min"=min(Perc.1.val, na.rm=TRUE), "max"=max(Perc.1.val, na.rm=TRUE),
            "mean"=mean(Perc.1.val, na.rm=TRUE), "median"=median(Perc.1.val, na.rm=TRUE),
            "sd"=sd(Perc.1.val, na.rm=TRUE), "iqr"=IQR(Perc.1.val, na.rm=TRUE));
knitr::kable(as.data.frame(d), digits=c(NA, NA, 2, 2, 2, 2, 2, 2), row.names=FALSE, caption="Summaries of the probability (as percent, %) to reconstruct a '1' at the root for each character across all trees and runs.");
```


### Model comparisons

#### "B" trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Assemble the likelihoods:
likelihoods.trees <- list("marginal"=rbind(m00.trees.B$marg.likelihoods, m11.trees.B$marg.likelihoods, 
                                           r00.trees.B$marg.likelihoods, r11.trees.B$marg.likelihoods, 
                                           t00.trees.B$marg.likelihoods, b00.trees.B$marg.likelihoods),
                          "posterior"=rbind(m00.trees.B$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")], 
                                            m11.trees.B$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")],
                                            r00.trees.B$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")], 
                                            r11.trees.B$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")],
                                            t00.trees.B$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")], 
                                            b00.trees.B$posterior.vars[,c("Model", "Posterior", "Likelihood", "Prior")]));
```

Pseudo model comparison using likelihood, posterior and prior (works for all models):

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=7, fig.cap="(Pseudo) model comparison using ln(likelihood)."}
d <- melt(likelihoods.trees$posterior, id.vars="Model");
ggplot(d, aes(x=Model, y=value, color=Model)) + ylab("Value") +
  geom_boxplot() + 
  facet_grid(. ~ variable) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL;

# Pairwise comparions:
m1 <- aov(Likelihood ~ Model, data=likelihoods.trees$posterior);
knitr::kable(TukeyHSD(m1)$Model, digits=c(2, 2, 2, 4), caption="Pairwise differences in ln(likelihood) between models (showing Tukey's HSD multiple testing correction).");
```

Proper model comparison using Bayes Factors (does not work for all models):

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=7, fig.cap="Model comparison using ln(marginal likelihood)."}
d <- likelihoods.trees$marginal;
ggplot(d, aes(x=Model, y=Marginal_Likelihood, color=Model)) + ylab("ln(marginal likelihood)") +
  geom_boxplot() + 
  facet_grid(. ~ Method) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  NULL;
```

Pairwise BFs for the corresponding individual trees, both as actual numbers (in the format: 'tree_id (BF)') and as plots (the horizontal lines represent the various degrees of strength of evidence for ln(BF): black = 0, between red and black = 'Barely worth mentioning', between red and blue = 'Substantial', between blue and dark green = 'Strong' and beyond dark green = 'Decisive'; positive favours the first model, negative the second model):

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis', fig.width=4, fig.height=4}
# Pairwise comparions:
BFs <- list();
m <- unique(likelihoods.trees$marginal$Model);
for(i in 1:(length(m)-1) )
{
  for(j in (i+1):length(m) )
  {
    cat(paste0("  - **",as.character(m[i]),"** - **",as.character(m[j]),"**:\n\n"));
    si <- likelihoods.trees$marginal$Model == m[i];
    sj <- likelihoods.trees$marginal$Model == m[j];
    shared.trees <- intersect(likelihoods.trees$marginal$Tree[ si ], 
                              likelihoods.trees$marginal$Tree[ sj ]);
    tmp <- lapply(unique(likelihoods.trees$marginal$Method), 
                  function(s)
                    {
                      cat(paste0("    + ",s,": "));
                      tmp <- vapply(shared.trees, 
                                     function(tree)
                                       { 
                                        st <- (likelihoods.trees$marginal$Tree == tree & likelihoods.trees$marginal$Method == s);
                                        return (likelihoods.trees$marginal$Marginal_Likelihood[si & st] - likelihoods.trees$marginal$Marginal_Likelihood[sj & st]);
                                       }, numeric(1));
                      names(tmp) <- shared.trees;
                      cat(paste0(names(tmp)," (",round(tmp,2),")", collapse=", ")); cat("\n");
                      return (tmp);
                    });
    names(tmp) <- unique(likelihoods.trees$marginal$Method);
    BFs[[length(BFs)+1]] <- tmp; names(BFs)[length(BFs)] <- paste0(m[i],"-",m[j]);
    cat("\n");
    
    d <- as.data.frame(do.call(rbind,tmp)); d$Method <- rownames(d); d <- melt(d, id.vars="Method");
    p <- ggplot(d, aes(x=Method, y=value, color=Method)) + 
      geom_hline(yintercept=0, color="black", linetype="solid") +
      geom_hline(yintercept=c(-1.16, 1.16), color="red", linetype="dotted") +
      geom_hline(yintercept=c(-2.30, 2.30), color="blue", linetype="dotted") +
      geom_hline(yintercept=c(-4.60, 4.60), color="darkgreen", linetype="dotted") +
      geom_boxplot() + geom_point(alpha=0.5) + ylab(paste0("ln(Bayes Factor) for ",as.character(m[i])," vs. ",as.character(m[j]))) + 
      NULL;
    print(p);
    cat("\n\n");
  }
}
```


### Summary

Individual models: 

  - **M00**: 100 trees (20 with 2 runs and marginal likelihoods), very good convergence for most trees, generally similar param estimates, ancestral states ~ 0% "1" median ~ 0% IQR, `r12` ~ 0.36 median ~ 0.3
  - **M11**: 100 trees (20 with 2 runs and marginal likelihoods), generally good convergence except for some trees for `site_rates` (which seem flipped between trees) and for `alpha_site_rates`, most params seem similar (`site_rates` seem flipped in some cases), ancestral states ~ 0% "1" median ~0% IQR, `alpha_site_rates` varies hugely, `beta_cats_rates` ~ 0.7 median ~ 1.4 IQR, `cats_r12` one very low and one very high
  - **R00**: 20 (20 with 2 runs and marginal likelihoods), very good convergence for most trees, generally similar param estimates, ancestral states < 5% "1" median ~ 20% IQR (except char3 with median 25% and 40 IQR), `r12` ~ 0.6 median ~ 1 IQR, `n_cats` ~ 8 median ~ 3 IQR (between 1 and 22)
  - **R11**: 19 trees (19 with 2 runs and 18 with marginal likelihoods), very good convergence for most trees, generally similar param estimates, ancestral states ~ 0% "1" median ~ 0% IQR, `alpha_site_rates` very consistent ~ 0.17 median ~ 0.1 IQR, `beta_cats_rates` very consistent ~ 0.7 median ~ 1 IQR, `cats_r12` very consistent (~0.2 and ~0.8), `n_cats` ~ 6 median ~ 6 IQR (between 1 and 19)
  - **T00**: 9 trees (8 with 2 runs and 0 with marginal likelihoods), very good convergence for most trees, generally similar param estimates, ancestral states ~ 50% "1" median ~ 2% IQR, `r12` ~ 2.3 median ~ 0.3, `n_cats` = 1, `concentration` ~ 0.18 median ~ 0.18 IQR 
  - **B00**: 9 trees (5 with 2 runs and 0 with marginal likelihoods), very good convergence for most trees, generally similar param estimates, ancestral states ~ 0% "1" median ~ 0% IQR, `r12` ~ 0.3 median ~ 0.2, `n_cats` ~ 2 median ~ 0 IQR (between 1 and 3), `concentration` ~ 0.28 median ~ 0.24 IQR, `base_rate` ~ 0.01 median ~ 0 IQR 

Model (pseudo-)comparison: 

  - **T00** is clearly the worst by far, **M00**, **R00** and **M11** next, and **R11** the best, with **B00** apparently comparable to **R11**.


### Interpretation

#### Consistency and differences between trees

Within the limits of the rather small sample of trees individually explored here (especially for **TU** and **TUBR**), there seems to be general agreement between individual trees for most parameters of interest here (especially the root state estimates and the number of branch rate categories), but there are, indeed, also some differences, depending on the model (e.g., for the models using two site rate categories, the two rate parameters `cats_r12` may be "flipped" between trees, with  and their hyperparameters `beta_cats_rates` and especially `alpha_site_rates`, reflecting this variation).
This variation between trees seems to confirm the hypothesis that the tree sampling mechanism does not work well enough for the models where there is important variation between trees in the optimal parameter values, making the jump between trees highly improbable. 

#### The original T&U model is deviant ...

The original T&U model simultaneously fits the data *the worst* and produces the *most deviant* estimates of the ancestral states, `n_cats` and `r12`.

#### ... but adding base rate back greatly improves it

Again, adding back the base rate to the original T&U model (as in the original `RevBayes` tutorial from which this was inspired) simultaneously:

  - *improves its fit* to the data (on average, from about ~ -290 for **T00** to ~ -200 for **B00** for 1 extra numeric parameter, corresponding to a $\Delta AIC \approx (2k - 2 \times (-290)) - (2(k+1) - 2 \times (-200))$ = $2 \times 290 -2 - 2 \times 200$ = $178$ AIC points), possibly making it the top (or at least not much worse than the best) model,
  - results in *ancestral state estimates at the root* of overwhelming absence of labiodentals (~0% reconstructed presence; in line with the original paper and the other models) from the argubaly wrong 50% for the original T&U model,
  - *increases `n_cats`* from 1 to 2, more in line with our original results and the other models.

Thus, unless theoretically strongly motivated, excluding the base rate (as done by the original T&U model) strongly distorts the results.

#### Other models (**MC** and **RMC**)

The simplest molecular clock **MC** and its relaxed variant **RMC** are pretty bad fits to the data (but not as bad as the original **TU**), but adding better root models and allowing between-site variation greatly improves the fit of **RMC**.

#### Probability of labiodentals at the root and changes in rates

All models except the original T&U reconstruct an *absence of labiodentals at the root* (some -- including **B00** -- with almost 100% probability) and a number of *branch rate categories* clearly greater than 1.



## Summary and conclusions

Thus, the original T&U model does not fit the data well (in fact, it produces pretty much the worst fit of all the models we tested) and its results are deviant when compared to the other models (especially when it comes to the number of branch rate categories, `n_cats`, considered by T&U as the most important measure in their response).

Interestingly, adding back to the T&U model a base rate parameter for the whole tree (as in the `RevBayes` tutorial apparently used as inspiration) both improves the fit to the data and "normalizes" the parameter estimates (including `n_cats` and the estimates of the probabilities that the root note that various types of labiodentals) to be more in line with the other models.

Adding to this the fact that the model is very complex (in its use of a Drichlet Process Prior) and breaks the `RevBayes` mechanism for computing Bayes Factors (making it thus impossible to properly compare to the other models), makes us conclude that the T&U model is unjustified for the case at hand (also including in its theoretical assumptions) and a suboptimal base for critisicing our paper.

Moreover, the tree sampling mechanism used by T&U seems to work well only when the model fits the trees equally well (or, rather, equally bad), allowing the mechanism to easily jump between trees; however, if the model fits differently different trees, these jumps become very improbable and the MCMC becomes stuck in an individual tree (one theoreitcal solution would be to allow many more moves across all parameters in each iteration, but this did not seem to produce any better results during our explorations). 

Turning to the other models, most fit the data at least as well as the original T&U and, focusing on those that fit the data sensibly better, all find a *very low probability that the root node had labiodentls* (interestingly, *character 3* consistently seems to have a much higher probability of being present) and overwhelimingly find *more than one branch rate category*.
This strongly supports our original results (an *absence of labiodentals at the origin of Indo-European languages*) and, if we accept the T&U view that there must have been a change in the branch rates if our hypothesis were right, we do *find an overwheling support for more than one category of branch rates* as well.




